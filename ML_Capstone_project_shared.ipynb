{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "private_outputs": true,
      "provenance": [],
      "collapsed_sections": [
        "GF8Ens_Soomf",
        "0wOQAZs5pc--",
        "K5QZ13OEpz2H",
        "lQ7QKXXCp7Bj",
        "448CDAPjqfQr",
        "KSlN3yHqYklG",
        "t6dVpIINYklI",
        "ijmpgYnKYklI",
        "-JiQyfWJYklI",
        "EM7whBJCYoAo",
        "fge-S5ZAYoAp",
        "85gYPyotYoAp",
        "RoGjAbkUYoAp",
        "4Of9eVA-YrdM",
        "iky9q4vBYrdO",
        "F6T5p64dYrdO",
        "y-Ehk30pYrdP",
        "bamQiAODYuh1",
        "QHF8YVU7Yuh3",
        "GwzvFGzlYuh3",
        "qYpmQ266Yuh3",
        "OH-pJp9IphqM",
        "bbFf2-_FphqN",
        "_ouA3fa0phqN",
        "Seke61FWphqN",
        "PIIx-8_IphqN",
        "t27r6nlMphqO",
        "r2jJGEOYphqO",
        "b0JNsNcRphqO",
        "BZR9WyysphqO",
        "jj7wYXLtphqO",
        "eZrbJ2SmphqO",
        "rFu4xreNphqO",
        "YJ55k-q6phqO",
        "gCFgpxoyphqP",
        "OVtJsKN_phqQ",
        "lssrdh5qphqQ",
        "U2RJ9gkRphqQ",
        "1M8mcRywphqQ",
        "tgIPom80phqQ",
        "JMzcOPDDphqR",
        "x-EpHcCOp1ci",
        "X_VqEhTip1ck",
        "8zGJKyg5p1ck",
        "PVzmfK_Ep1ck",
        "n3dbpmDWp1ck",
        "ylSl6qgtp1ck",
        "ZWILFDl5p1ck",
        "M7G43BXep1ck",
        "Ag9LCva-p1cl",
        "E6MkPsBcp1cl",
        "2cELzS2fp1cl",
        "3MPXvC8up1cl",
        "NC_X3p0fY2L0",
        "UV0SzAkaZNRQ",
        "YPEH6qLeZNRQ",
        "q29F0dvdveiT",
        "EXh0U9oCveiU",
        "22aHeOlLveiV",
        "g-ATYxFrGrvw",
        "Yfr_Vlr8HBkt",
        "8yEUt7NnHlrM",
        "tEA2Xm5dHt1r",
        "I79__PHVH19G",
        "Ou-I18pAyIpj",
        "fF3858GYyt-u",
        "4_0_7-oCpUZd",
        "hwyV_J3ipUZe",
        "3yB-zSqbpUZe",
        "dEUvejAfpUZe",
        "Fd15vwWVpUZf",
        "bn_IUdTipZyH",
        "49K5P_iCpZyH",
        "Nff-vKELpZyI",
        "kLW572S8pZyI",
        "dWbDXHzopZyI",
        "yLjJCtPM0KBk",
        "xiyOF9F70UgQ",
        "7wuGOrhz0itI",
        "id1riN9m0vUs",
        "578E2V7j08f6",
        "89xtkJwZ18nB",
        "67NQN5KX2AMe",
        "Iwf50b-R2tYG",
        "GMQiZwjn3iu7",
        "WVIkgGqN3qsr",
        "XkPnILGE3zoT",
        "Hlsf0x5436Go",
        "mT9DMSJo4nBL",
        "c49ITxTc407N",
        "OeJFEK0N496M",
        "9ExmJH0g5HBk",
        "cJNqERVU536h",
        "k5UmGsbsOxih",
        "T0VqWOYE6DLQ",
        "qBMux9mC6MCf",
        "-oLEiFgy-5Pf",
        "C74aWNz2AliB",
        "2DejudWSA-a0",
        "pEMng2IbBLp7",
        "rAdphbQ9Bhjc",
        "TNVZ9zx19K6k",
        "nqoHp30x9hH9",
        "rMDnDkt2B6du",
        "yiiVWRdJDDil",
        "1UUpS68QDMuG",
        "kexQrXU-DjzY",
        "T5CmagL3EC8N",
        "BhH2vgX9EjGr",
        "qjKvONjwE8ra",
        "P1XJ9OREExlT",
        "VFOzZv6IFROw",
        "TIqpNgepFxVj",
        "VfCC591jGiD4",
        "OB4l2ZhMeS1U",
        "ArJBuiUVfxKd",
        "4qY1EAkEfxKe",
        "PiV4Ypx8fxKe",
        "TfvqoZmBfxKf",
        "dJ2tPlVmpsJ0",
        "JWYfwnehpsJ1",
        "-jK_YjpMpsJ2",
        "HAih1iBOpsJ2",
        "zVGeBEFhpsJ2",
        "bmKjuQ-FpsJ3",
        "Fze-IPXLpx6K",
        "7AN1z2sKpx6M",
        "9PIHJqyupx6M",
        "_-qAgymDpx6N",
        "Z-hykwinpx6N",
        "h_CCil-SKHpo",
        "cBFFvTBNJzUa",
        "HvGl1hHyA_VK",
        "EyNgTHvd2WFk",
        "KH5McJBi2d8v",
        "iW_Lq9qf2h6X",
        "-Kee-DAl2viO",
        "gCX9965dhzqZ",
        "gIfDvo9L0UH2"
      ],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/s07376/EDA-project/blob/main/ML_Capstone_project_shared.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Project Name**    -**Bike sharing demand prediction**\n",
        "\n"
      ],
      "metadata": {
        "id": "vncDsAP0Gaoa"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### **Project Type**    - Regression\n",
        "##### **Contribution**    - Individual\n"
      ],
      "metadata": {
        "id": "beRrZCGUAJYm"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Project Summary -**"
      ],
      "metadata": {
        "id": "FJNUwmbgGyua"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "\n",
        "*   This ML project is dedicated to improving public mobility and convenience through the implementation of bike-sharing programs in metropolitan areas. With the aim of ensuring a consistent supply of bikes for rental, we tackle the challenge of predicting demand based on historical data. By leveraging factors such as temperature and time, we employ advanced machine learning techniques to forecast the bike-sharing program's usage in Seoul.\n",
        "*   With a dataset containing approximately **8760 records and 14 attributes**, we embarked on an extensive data analysis journey. Through exploratory data analysis (EDA), we gained valuable insights and prepared the data for modeling. Cleaning processes were applied to eliminate outliers and handle null values, while appropriate transformations were employed to ensure compatibility with machine learning algorithms.\n",
        "*   Subsequently, the cleaned and scaled data was fed into **11 diverse models**, allowing us to evaluate their performance using multiple metrics. Hyperparameter tuning was conducted to optimize the models and ensure accurate predictions.\n",
        "*   In our evaluation, we prioritize metrics such as the R2 score and RMSE score. The R2 score, being scale-independent, enables us to compare models with different target variables or units of measurement. This provides a robust framework for assessing model performance across various problem domains.\n",
        "*   By accurately forecasting bike-sharing demand, we aim to enhance resource allocation, reduce wait times, and elevate the overall user experience.\n"
      ],
      "metadata": {
        "id": "F6v_1wHtG2nS"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **GitHub Link -**"
      ],
      "metadata": {
        "id": "w6K7xa23Elo4"
      }
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "h1o69JH3Eqqn"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Problem Statement**\n"
      ],
      "metadata": {
        "id": "yQaldy8SH6Dl"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**The \"Bike Sharing Demand Prediction\" project addresses the challenge faced by bike sharing companies in accurately forecasting and meeting the fluctuating demand for bike rentals. The unpredictable nature of bike rental demand poses difficulties in managing fleet size, allocating resources, and providing optimal customer service. Without a reliable demand prediction system, bike sharing companies often struggle to ensure a sufficient number of bikes are available during peak periods, resulting in frustrated customers and missed revenue opportunities. Conversely, overestimating demand leads to surplus bikes and unnecessary operational costs. Therefore, the problem at hand is to develop a robust machine learning model that can accurately forecast bike rental demand, enabling companies to optimize fleet management, allocate resources efficiently, and deliver an exceptional user experience while maximizing profitability.**"
      ],
      "metadata": {
        "id": "DpeJGUA3kjGy"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# ***Let's Begin !***"
      ],
      "metadata": {
        "id": "O_i_v8NEhb9l"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## ***1. Know Your Data***"
      ],
      "metadata": {
        "id": "HhfV-JJviCcP"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Import Libraries"
      ],
      "metadata": {
        "id": "Y3lxredqlCYt"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Import Libraries\n",
        "\n",
        "# Import Libraries and modules\n",
        "\n",
        "# libraries that are used for analysis and visualization\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "%matplotlib inline\n",
        "import seaborn as sns\n",
        "import missingno as msno\n",
        "from scipy import stats as st\n",
        "\n",
        "pd.set_option('display.max_columns', 500)\n",
        "\n",
        "plt.style.use('ggplot')\n",
        "\n",
        "# to import datetime library\n",
        "from datetime import datetime\n",
        "import datetime as dt\n",
        "\n",
        "# libraries used to pre-process\n",
        "from sklearn import preprocessing, linear_model\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.preprocessing import OneHotEncoder\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "\n",
        "# libraries used to implement models\n",
        "from sklearn.linear_model import LinearRegression, Ridge, Lasso, ElasticNet\n",
        "from sklearn.neighbors import KNeighborsRegressor\n",
        "from sklearn.tree import DecisionTreeRegressor\n",
        "from sklearn.ensemble import RandomForestRegressor, ExtraTreesRegressor, GradientBoostingRegressor, AdaBoostRegressor\n",
        "from sklearn.svm import SVR\n",
        "from xgboost import XGBRegressor\n",
        "import xgboost as xgb\n",
        "from lightgbm import LGBMRegressor\n",
        "from sklearn.model_selection import GridSearchCV, RandomizedSearchCV\n",
        "\n",
        "# libraries to evaluate performance\n",
        "from sklearn import metrics\n",
        "from sklearn.metrics import mean_squared_error, r2_score, accuracy_score, mean_absolute_error\n",
        "\n",
        "# Library of warnings would assist in ignoring warnings issued\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "# to set max column display\n",
        "pd.pandas.set_option('display.max_columns',None)"
      ],
      "metadata": {
        "id": "M8Vqi-pPk-HR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Dataset Loading"
      ],
      "metadata": {
        "id": "3RnN4peoiCZX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#mounting the drive\n",
        "\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "id": "4CkvbW_SlZ_R"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Load Seoul Bike Dataset\n",
        "df=pd.read_csv(\"/content/drive/MyDrive/Capstone projects/Module_6_Machine learning/SeoulBikeData.csv\",encoding= 'unicode_escape')\n",
        "df.head()"
      ],
      "metadata": {
        "id": "7-NbbAJcpjKE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Dataset First View"
      ],
      "metadata": {
        "id": "x71ZqKXriCWQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Dataset First Look\n",
        "# Display the first 5 rows\n",
        "df.head()"
      ],
      "metadata": {
        "id": "LWNFOSvLl09H"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Check last five records\n",
        "df.tail()"
      ],
      "metadata": {
        "id": "v6j-Qiibr9fc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#check random sample\n",
        "df.sample(5)"
      ],
      "metadata": {
        "id": "UzQrtWq8r9HZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Dataset Rows & Columns count"
      ],
      "metadata": {
        "id": "7hBIi_osiCS2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Dataset Rows & Columns count\n",
        "\n",
        "# Dimensions of the dataset\n",
        "df.shape"
      ],
      "metadata": {
        "id": "Kllu7SJgmLij"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "There are 8760 rows and 14 columns in this dataset."
      ],
      "metadata": {
        "id": "B5rNNJwULAwI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Name of columns in the data\n",
        "df.columns"
      ],
      "metadata": {
        "id": "onBXoZsRshAZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Dataset Information"
      ],
      "metadata": {
        "id": "JlHwYmJAmNHm"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Dataset Info\n",
        "# Get information about the dataset\n",
        "df.info()"
      ],
      "metadata": {
        "id": "e9hRXRi6meOf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Observation:**\n",
        "\n",
        "* Float64 datatype: 6 columns ie Temperature(°C), Wind speed (m/s, Dew point temperature(°C), Solar Radiation(MJ/m2), Rainfall(mm), Snowfall(cm) & Seasons.\n",
        "\n",
        "* Int64 datatype: 4 columns ie Rented Bike, Count, Hour, Humidity(%) & Visibility(10m).\n",
        "\n",
        "*  Object datatype: 4 columns ie Date, Seasons, Holidays & Functioming Day.**\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "oMQtbLCCtssJ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Duplicate Values"
      ],
      "metadata": {
        "id": "35m5QtbWiB9F"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Dataset Duplicate Value Count\n",
        "\n",
        "print('The number of duplicated values in each column:' , df.duplicated().sum())"
      ],
      "metadata": {
        "id": "1sLdpKYkmox0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Unique values"
      ],
      "metadata": {
        "id": "I6laQEiYugDC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Number of unique values in each columns\n",
        "df.nunique()"
      ],
      "metadata": {
        "id": "bPzG9w5wugDE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**From the above result, it is observed that this datasets contains bike rental data of 1 year (since there are 365 unique values in a Date column)**"
      ],
      "metadata": {
        "id": "Okr7aM_PuuAr"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Missing Values/Null Values"
      ],
      "metadata": {
        "id": "PoPl-ycgm1ru"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Missing Values/Null Values Count\n",
        "# Check for missing values\n",
        "\n",
        "df.isnull().sum()"
      ],
      "metadata": {
        "id": "GgHWkxvamxVg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Visualizing the missing values\n",
        "\n",
        "msno.matrix(df)"
      ],
      "metadata": {
        "id": "3q5wnI3om9sJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**From the above results, it is evident that there are no missing values in the dataset .**"
      ],
      "metadata": {
        "id": "3LA0EmxWeqAx"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### What did you know about your dataset?"
      ],
      "metadata": {
        "id": "H0kj-8xxnORC"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "* The dataset contains 8760 rows and 14 columns.\n",
        "* There are 6 columns of datatype float64, 4 columns of datatype int64 and 4 columns of datatype object.\n",
        "* There are no missing and duplicate values in the dataset.\n",
        "* The dataset contains bike rental data of 1 year.\n",
        "* Input features: Date, Hour, Temperature(°C), Humidity(%), Wind speed (m/s),Visibility (10m), Dew point temperature(°C), Solar Radiation (MJ/m2), Rainfall(mm), Snowfall (cm), Season, Holiday & Functioning Day\n",
        "* Target feature: Rented Bike Count"
      ],
      "metadata": {
        "id": "gfoNAAC-nUe_"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## ***2. Understanding Your Variables***"
      ],
      "metadata": {
        "id": "nA9Y7ga8ng1Z"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Dataset Columns\n",
        "df.columns.tolist()"
      ],
      "metadata": {
        "id": "j7xfkqrt5Ag5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Dataset Describe\n",
        "df.describe().T"
      ],
      "metadata": {
        "id": "DnOaZdaE5Q5t"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "(df[\"Rainfall(mm)\"]<1).sum()"
      ],
      "metadata": {
        "id": "j4skhqeodFWl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "(df[\"Rainfall(mm)\"]>1).sum()"
      ],
      "metadata": {
        "id": "72i1sz8heN16"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "(df[\"Snowfall (cm)\"]<1).sum()"
      ],
      "metadata": {
        "id": "IunSSFWdeIg-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "(df[\"Snowfall (cm)\"]>1).sum()"
      ],
      "metadata": {
        "id": "X4sIEiqxeTBN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "*   **Majority of values for Rainfall & Snowfall are below 1**"
      ],
      "metadata": {
        "id": "Ms9AdQ8qeh3W"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df['Seasons'].value_counts()"
      ],
      "metadata": {
        "id": "epTWzjKSe-O3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df['Functioning Day'].value_counts()"
      ],
      "metadata": {
        "id": "6zDs5bOqfAuC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df['Holiday'].value_counts()"
      ],
      "metadata": {
        "id": "n34RNF6lfDSZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Variables Description"
      ],
      "metadata": {
        "id": "PBTbrJXOngz2"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The dataset contains weather information (Temperature, Humidity, Windspeed, Visibility, Dewpoint, Solar radiation, Snowfall, Rainfall), the number of bikes rented per hour and date information.\n",
        "\n",
        "Attribute Information:\n",
        "\n",
        "\n",
        "\n",
        "*   Date: The specific calendar date for the bike rental record.\n",
        "*   Rented Bike Count: The number of bikes rented during a specific time interval.\n",
        "*   Temperature: The temperature in Celsius at the time of the bike rental.\n",
        "*   Humidity: The relative humidity percentage at the time of the bike rental.\n",
        "*   Wind Speed: The speed of the wind in meters per second at the time of the bike rental.\n",
        "*   Visibility: The visibility in meters at the time of the bike rental.\n",
        "*   Dew Point Temperature: The temperature at which air becomes saturated and dew forms at the time of the bike rental.\n",
        "*   Solar Radiation: The amount of solar radiation in mega-joules per square meter at the time of the bike rental.\n",
        "*   Rainfall: The amount of rainfall in millimeters at the time of the bike rental.\n",
        "*   Snowfall: The amount of snowfall in centimeters at the time of the bike rental.\n",
        "*   Seasons: The four seasons (Spring, Summer, Autumn, Winter) corresponding to the bike rental record.\n",
        "*   Holiday: A categorical variable indicating whether the day of the bike rental record is a holiday or not. It has two possible values: \"Holiday\" and \"No Holiday\". The \"Holiday\" value represents a day that is recognized as a holiday, while the \"No Holiday\" value represents a regular day that is not a designated holiday.\n",
        "*   Functioning Day: A categorical variable indicating whether the bike rental service was functioning on the day of the record. It has two possible values: \"Yes\" and \"No\". The \"Yes\" value indicates that the bike rental service was operational and functioning normally on that day. Conversely, the \"No\" value indicates that the bike rental service was not operating, potentially due to maintenance, strikes, or other reasons.\n"
      ],
      "metadata": {
        "id": "aJV4KIxSnxay"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Check Unique Values for each variable."
      ],
      "metadata": {
        "id": "u3PMJOP6ngxN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Check Unique Values for each variable.\n",
        "for i in df.columns.to_list():\n",
        "  print('Number of unique values in', i, 'is', df[i].nunique())"
      ],
      "metadata": {
        "id": "zms12Yq5n-jE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Converting Date column of datatype Object to Datetime datatype\n",
        "df['Date'] = pd.to_datetime(df['Date'], dayfirst = True)"
      ],
      "metadata": {
        "id": "fBD_07iXim_z"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Write your code to make your dataset analysis ready.\n",
        "\n",
        "# Extracting day name feature\n",
        "df['Day'] = df['Date'].dt.day_name()\n",
        "\n",
        "# Extracting month name feature\n",
        "df['Month'] = df['Date'].dt.month_name()\n",
        "\n",
        "# Extracting year feature\n",
        "df['Year'] = df['Date'].dt.year\n"
      ],
      "metadata": {
        "id": "wk-9a2fpoLcV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Dropping Date column\n",
        "df.drop(columns = ['Date'], inplace = True)"
      ],
      "metadata": {
        "id": "63jkKNiwkTHa"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Rename the complex columns name\n",
        "df = df.rename(columns={\n",
        "                                'Temperature(°C)':'Temperature',\n",
        "                                'Humidity(%)':'Humidity',\n",
        "                                'Wind speed (m/s)':'Wind Speed',\n",
        "                                'Visibility (10m)':'Visibility',\n",
        "                                'Dew point temperature(°C)':'Dew point temperature',\n",
        "                                'Solar Radiation (MJ/m2)':'Solar Radiation',\n",
        "                                'Rainfall(mm)':'Rainfall',\n",
        "                                'Snowfall (cm)':'Snowfall',\n",
        "                              })"
      ],
      "metadata": {
        "id": "s7Wk5oh_kYMh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#To observe the day, month , year columns and other changes made to the dataset\n",
        "df.sample(3)"
      ],
      "metadata": {
        "id": "VSiF8q5mkhmB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# convert Hour and Year columns from integer to object\n",
        "df['Hour'] = df['Hour'].astype('object')\n",
        "df['Year'] = df['Year'].astype('object')"
      ],
      "metadata": {
        "id": "yDXns3ddkhid"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 3. ***Exploratory data analysis***"
      ],
      "metadata": {
        "id": "dauF4eBmngu3"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**What is EDA?**\n",
        "\n",
        "- EDA stands for Exploratory Data Analysis. It is a crucial step in the data analysis process that involves exploring and understanding the characteristics, patterns, and relationships within a dataset. EDA aims to uncover insights, identify patterns, detect outliers, and gain a deeper understanding of the data before conducting further analysis or modeling."
      ],
      "metadata": {
        "id": "EfcsDUF6nOiD"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df.info()"
      ],
      "metadata": {
        "id": "LMNHREngnZqt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **3.1 Numerical & categorical variable**"
      ],
      "metadata": {
        "id": "0EoAb2ksoDsc"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Dividing data into numerical and categorical features\n",
        "\n",
        "categorical_features = df.select_dtypes(include = 'object')\n",
        "numerical_features = df.select_dtypes(exclude = 'object')"
      ],
      "metadata": {
        "id": "Y2txHS3gn9oN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "categorical_features.head(2)"
      ],
      "metadata": {
        "id": "4yP6QibToboN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "numerical_features.head(2)"
      ],
      "metadata": {
        "id": "ffCd92vBod9E"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **3.2 Univariate analysis**"
      ],
      "metadata": {
        "id": "EHcXhTRAoyBh"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **3.2.1 Data distribution of numeric features**"
      ],
      "metadata": {
        "id": "3XVNElq4pLMP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# figsize\n",
        "plt.figure(figsize=(15,10))\n",
        "\n",
        "# title\n",
        "plt.suptitle('Data Distribution of Numeric Features', fontsize = 20, fontweight = 'bold', y=1.02)\n",
        "\n",
        "for i, col in enumerate(numerical_features):\n",
        "  # subplots 3 rows and 3 columns\n",
        "  plt.subplot(3, 3, i+1 )\n",
        "\n",
        "  # dist plot\n",
        "  sns.distplot(df[col])\n",
        "  plt.axvline(df[col].mean(), color='magenta', linestyle='dashed', linewidth=2)\n",
        "  plt.axvline(df[col].median(), color='cyan', linestyle='dashed', linewidth=2)\n",
        "\n",
        "  plt.title(col)\n",
        "  plt.tight_layout()"
      ],
      "metadata": {
        "id": "48dludyNpSLp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Observations:**\n",
        "*   Right-skewed features are: Rented Bike Count, Wind speed, Solar Radiation, Rainfall & Snowfall.\n",
        "*   Left-skewed features: Visibility & Dew point temperature\n"
      ],
      "metadata": {
        "id": "lMsAQa8AqojX"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **3.2.2 Outlier analysis of numerical features**"
      ],
      "metadata": {
        "id": "LvJf78Soq-4m"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# figsize\n",
        "plt.figure(figsize = (15,10))\n",
        "\n",
        "# title\n",
        "plt.suptitle('Outlier Analysis of Numeric features', fontsize = 20, fontweight='bold', y=1.02)\n",
        "\n",
        "for i, col in enumerate(numerical_features):\n",
        "  # subplots 3 rows, 3 columns\n",
        "  plt.subplot(3,3, i+1)\n",
        "\n",
        "  # boxplots\n",
        "  sns.boxplot(numerical_features[col])\n",
        "\n",
        "  plt.title(col)\n",
        "  plt.tight_layout()"
      ],
      "metadata": {
        "id": "wz_40fbFqiW_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Observations:**\n",
        "*   Outliers are visible in Rented Bike Count, Wind Speed, Solar Radiation, Rainfall & Snowfall.\n",
        "*   The columns like Temperature, Humidity, Visibility & Dew point temperature do not contain any outliers.\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "5UFyCmw2sYk0"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **3.2.3 Univariate analysis of categorical features**"
      ],
      "metadata": {
        "id": "i0kd9lH7tRTw"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# figure\n",
        "plt.figure(figsize = (20,8))\n",
        "\n",
        "# title\n",
        "plt.suptitle('Univariate Analysis of Categorical Features', fontsize = 20, fontweight = 'bold', y = 1.02)\n",
        "\n",
        "for i, col in enumerate(categorical_features):\n",
        "  # subplots of\n",
        "  plt.subplot(3,3, i+1)\n",
        "\n",
        "  # Countplots\n",
        "  sns.countplot(x = categorical_features[col])\n",
        "\n",
        "  plt.xticks(rotation ='vertical')\n",
        "  plt.title(col)\n",
        "  plt.tight_layout()"
      ],
      "metadata": {
        "id": "5i9YhRwFtb0I"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Observations:**\n",
        "- Every hour has an equal number of counts in the dataset.\n",
        "- Every season has almost equal number of counts.\n",
        "- Dataset has more records of No holiday than a holiday which is obvious as most of the days are working days.\n",
        "- Dataset has more records of Functioning Day than no functioning day which is obvious as most of the days are working days.\n",
        "- Except Friday, other Days have equal number of counts in the dataset.\n",
        "- Months like April, June, September, November & February have a slightly low number of count comparted to other months.\n",
        "- More data was colected in the year 2018 than 2017."
      ],
      "metadata": {
        "id": "FAX96SfRuExP"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **3.3 Multivariate & Bivariate analysis**"
      ],
      "metadata": {
        "id": "Bpt8orYiunSQ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **3.3.1 Analysis between target variable and numerical features**"
      ],
      "metadata": {
        "id": "KgbYL1zPvFwD"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Identify patterns and trends in numerical features\n",
        "\n",
        "plt.suptitle('Bivariate Analysis of Numerical features', fontsize=20, fontweight='bold', y=1.02)\n",
        "\n",
        "\n",
        "for i in numerical_features:\n",
        "  if i==\"Rented Bike Count\":\n",
        "    pass\n",
        "  else:\n",
        "    plt.figure(figsize=(15,6))\n",
        "    sns.lineplot(x= i, y='Rented Bike Count', data = numerical_features, palette='Grouped')\n",
        "    plt.title(f\"Bike Demand over {i}\");\n",
        "    print('\\n')\n",
        "    plt.xticks(rotation = 45)\n"
      ],
      "metadata": {
        "id": "L6wdq30NvNYE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "plt.figure(figsize = (15, 10))\n",
        "\n",
        "# title\n",
        "plt.suptitle('Bivariate Analysis of Numerical features', fontsize=20, fontweight='bold', y=1.02)\n",
        "\n",
        "for index, col in enumerate(numerical_features):\n",
        "  if col==\"Rented Bike Count\":\n",
        "    pass\n",
        "  else:\n",
        "    # subplots of 3 rows and 3 columns\n",
        "    plt.subplot(3,3, index+1)\n",
        "\n",
        "    # line plots\n",
        "    sns.scatterplot(x = numerical_features[col], y = numerical_features['Rented Bike Count'])\n",
        "\n",
        "    plt.title(f'Bike Demand Over {col}')\n",
        "    plt.xticks(rotation = 45)\n",
        "    plt.tight_layout()"
      ],
      "metadata": {
        "id": "YRGmieN6ujcj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **3.3.2 Bivariate Analysis of Categorical Features**"
      ],
      "metadata": {
        "id": "GXQzV4Bal6Cc"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Counting number of category present in each feature with respect to target feature\n",
        "\n",
        "# figsize\n",
        "plt.figure(figsize=(15,10))\n",
        "# title\n",
        "plt.suptitle('Bivariate Analysis of Categorical Features', fontsize=20, fontweight='bold', y=1.02)\n",
        "\n",
        "for i,col in enumerate(categorical_features):\n",
        "   # subplots of 3 rows and 3 columns\n",
        "  plt.subplot(3, 3, i+1)\n",
        "  a = df.groupby(col)[['Rented Bike Count']].mean().reset_index()\n",
        "\n",
        "  # barplot\n",
        "  sns.barplot(x=a[col], y=a['Rented Bike Count'])\n",
        "  # x-axis label\n",
        "  plt.title(f'Average bike rentals across {col}')\n",
        "  plt.xticks(rotation = 'vertical')\n",
        "  plt.tight_layout()"
      ],
      "metadata": {
        "id": "2om798j1mDZ4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Observations:\n",
        "\n",
        "*   Hours: The highest demand is in hours from say 7-10 and from 15-19. This could be the reason that in most of the metroploitan cities this is the peak office time and so more people would be renting bikes.\n",
        "\n",
        "*   Seasons: Summer season had the higest Bike Rent Count. People are more likely to rent bikes in summer. Bike rentals in winter is very less compared to other seasons.\n",
        "\n",
        "*   Holidays: High number of bikes were rented on No Holidays.\n",
        "\n",
        "*   Functioning Day: On 'No Functioning Day, zero bikes were rented. Hence, this column does not add value to our prediction, we can drop this column in the next steps.\n",
        "\n",
        "*   Day: Most of the bikes were rented on Weekdays compared to weekends.\n",
        "*   Month: From March Bike Rent Count started increasing and it was highest in June."
      ],
      "metadata": {
        "id": "aKQcgMBBnkA3"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **3.3.3 Multivariate analysis**"
      ],
      "metadata": {
        "id": "4cfYJTHxrnqy"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Analysing bike demand with respect to hour and different third value\n",
        "\n",
        "for i in categorical_features:\n",
        "  if i == 'Hour':\n",
        "    pass\n",
        "  else:\n",
        "    plt.figure(figsize=(15,8))\n",
        "    sns.lineplot(x= df[\"Hour\"], y= df['Rented Bike Count'], hue= df[i], marker ='o')\n",
        "    plt.title(f\"Bike Demand over Hour wrt to {i}\")\n",
        "  plt.show()"
      ],
      "metadata": {
        "id": "pU9PKSnvr3Ym"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Bar plot for seasonwise monthly distribution of Rented_Bike_Count\n",
        "fig,ax=plt.subplots(figsize=(15,8))\n",
        "sns.barplot(x='Month',y='Rented Bike Count',data= df, hue='Seasons',ax=ax);\n",
        "ax.set_title('Season-wise monthly Rented Bike Count');\n",
        "plt.show();"
      ],
      "metadata": {
        "id": "VlilB5JCtU1C"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        " **Observations:**\n",
        "*   The above regression plots for the numerical features indicate that the columns Temperature, Wind_speed, Visibility, Dew_point_temperature & Solar_Radiation are positively correlated with the target variable, ie , with an increase in these features results in an increase in rented bike count.\n",
        "*   On the other hand, Rainfall, Snowfall & Humidity are negatively correlated with the target variable, indicating that with an increase in these features results in a decrease in rented bike count.\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "n64Do7sZxXAr"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **4. Data cleaning**"
      ],
      "metadata": {
        "id": "C-J7fCvdxpcf"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "*   Data cleaning is the process of identifying and correcting or removing errors, inconsistencies, and inaccuracies in a dataset.\n",
        "*   It involves handling missing data, removing duplicates, addressing outliers, standardizing formats, resolving inconsistencies, and validating data.\n",
        "*   Data cleaning ensures that the data is accurate, complete, and reliable for analysis or machine learning purposes."
      ],
      "metadata": {
        "id": "eA40qi7rxwqC"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **4.1 Handling missing values**"
      ],
      "metadata": {
        "id": "esH9J55n3GMp"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Checking for missing values\n",
        "df.isnull().sum()"
      ],
      "metadata": {
        "id": "jyYZS1USxslw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**As we can see there are no null values present in our dataset and therefore we are good to go.**"
      ],
      "metadata": {
        "id": "2OCMw5I-38p7"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **4.2 Handling duplicate values**"
      ],
      "metadata": {
        "id": "I5pd0RQ34eS5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Checking for duplicate values\n",
        "df.duplicated().sum()"
      ],
      "metadata": {
        "id": "rs404Lum4cUj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "As we can see there are no duplicate values"
      ],
      "metadata": {
        "id": "jUBTp2j66q97"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **4.3 Handling outliers**"
      ],
      "metadata": {
        "id": "vyP62BqF7R2Z"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Creating a boxplot to detect columns with outliers\n",
        "# figsize\n",
        "plt.figure(figsize = (15,10))\n",
        "\n",
        "# title\n",
        "plt.suptitle('Outlier Analysis of Numerical features', fontsize = 20, fontweight='bold', y=1.02)\n",
        "\n",
        "for index , col in enumerate(numerical_features):\n",
        "  # subplots 3 rows, 3 columns\n",
        "  plt.subplot(3,3, index+1)\n",
        "\n",
        "  # boxplots\n",
        "  sns.boxplot(numerical_features[col])\n",
        "\n",
        "  plt.title(col)\n",
        "  plt.tight_layout()"
      ],
      "metadata": {
        "id": "3u3LD6ny6vpE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Here we can see that the columns that contain outliers are Rented Bike Count,Windspeed,Solar Radiation,Rainfall & Snowfall**"
      ],
      "metadata": {
        "id": "CIQzSD0A8SFs"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Creating a list of columns that contains outliers\n",
        "outlier_cols = ['Rented Bike Count', 'Wind Speed', 'Solar Radiation', 'Rainfall','Snowfall']\n",
        "outlier_cols"
      ],
      "metadata": {
        "id": "NTWe2RlF8bbl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def calculate_ranges(data, column):\n",
        "\n",
        "  # Skip categorical columns\n",
        "  if data[column].dtype == 'object':\n",
        "    return None, None\n",
        "  else:\n",
        "    # Calculate quartiles\n",
        "    Q1 = data[column].quantile(0.25)\n",
        "    Q3 = data[column].quantile(0.75)\n",
        "\n",
        "    # Calculate IQR\n",
        "    IQR = Q3 - Q1\n",
        "\n",
        "    # Calculate upper and lower ranges\n",
        "    upper_range = Q3 + 1.5 * IQR\n",
        "    lower_range = Q1 - 1.5 * IQR\n",
        "\n",
        "    return upper_range, lower_range"
      ],
      "metadata": {
        "id": "CIu-FRrz8iJg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "calculate_ranges(numerical_features, 'Rented Bike Count')"
      ],
      "metadata": {
        "id": "b8lwmXVi9qgN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Identify potential outliers\n",
        "plt.figure(figsize = (15,10))\n",
        "# plt.suptitle('Distribution of Numerical features with Potential Outliers', fontsize = 20, fontweight='bold', y=1.02)\n",
        "for index, col in enumerate(outlier_cols):\n",
        "\n",
        "\n",
        "  # Apply calculate_ranges function to get upper bound and lower bound\n",
        "  upper_bound, lower_bound = calculate_ranges(df, col)\n",
        "\n",
        "  # Identify potential outliers\n",
        "  outliers = df[(df[col] > upper_bound) | (df[col] < lower_bound)]\n",
        "\n",
        "# Visualize the potential outliers\n",
        "\n",
        "  plt.figure(figsize=(12, 12))\n",
        "\n",
        "  # subplots 3 rows, 3 columns\n",
        "  plt.subplot(3,3, index+1)\n",
        "  plt.hist(df[col], bins=30, color='lightblue', edgecolor='black', label='Data')\n",
        "  plt.hist(outliers[col], bins=10, color='red', edgecolor='black', label='Potential Outliers')\n",
        "  plt.xlabel(col)\n",
        "  plt.ylabel('Frequency')\n",
        "  plt.title(f\"Distribution of {col} with Potential Outliers\", fontsize = 10, fontweight='bold', y=1.02)\n",
        "  plt.legend()\n",
        "  plt.tight_layout()\n",
        "  plt.show()"
      ],
      "metadata": {
        "id": "kYfMGNSG9qVj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Create a function to count the total number of outliers in each column\n",
        "\n",
        "def count_outliers(data):\n",
        "    # Initialize a variable to store the total number of outliers\n",
        "    outlier_count = {}\n",
        "\n",
        "    # Loop through each column in the list containing outliers\n",
        "    for col in outlier_cols:\n",
        "\n",
        "        # Calculate the upper and lower ranges\n",
        "        upper_range, lower_range = calculate_ranges(data, col)\n",
        "\n",
        "        # Count the number of outliers in the column\n",
        "        outlier_count[col] = len(data[(data[col] > upper_range) | (data[col] < lower_range)])\n",
        "\n",
        "    return outlier_count"
      ],
      "metadata": {
        "id": "WW7XOmEs-gLU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Number of outliers in each column\n",
        "count_outliers(df)"
      ],
      "metadata": {
        "id": "EVS1QPCA-rwB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Observation:**\n",
        "\n",
        "It is not recommended to trim the entire outliers as we tend to lose many data points. Hence we are not simply removing the outlier instead of that we are using the clipping method."
      ],
      "metadata": {
        "id": "hH3hbqI6-3ub"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "num_features = ['Temperature', 'Humidity', 'Wind Speed', 'Visibility', 'Dew point temperature', 'Solar Radiation']"
      ],
      "metadata": {
        "id": "xMf5gU7V-8hZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Clipping Method:**In this method, we set a cap on our outliers data, which means that if a value is higher than or lower than a certain threshold, all values will be considered outliers. This method replaces values that fall outside of a specified range with either the minimum or maximum value within that range."
      ],
      "metadata": {
        "id": "3dL3HbGFEbCp"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# we are going to replace the datapoints with upper and lower bound of all the outliers\n",
        "\n",
        "def clip_outliers(bike_df):\n",
        "    #numerical_features = ['Temperature', 'Humidity', 'Wind Speed', 'Visibility', 'Dew point temperature', 'Solar Radiation']\n",
        "\n",
        "    for col in num_features:\n",
        "        # Using IQR method to define the range of upper and lower limits\n",
        "        q1 = bike_df[col].quantile(0.25)\n",
        "        q3 = bike_df[col].quantile(0.75)\n",
        "        iqr = q3 - q1\n",
        "        lower_bound = q1 - 1.5 * iqr\n",
        "        upper_bound = q3 + 1.5 * iqr\n",
        "\n",
        "        # Replacing the outliers with the upper and lower bounds\n",
        "        bike_df[col] = bike_df[col].clip(lower_bound, upper_bound)\n",
        "\n",
        "    return bike_df"
      ],
      "metadata": {
        "id": "B_FKteD2FE1T"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Creating a copy of dataset\n",
        "\n",
        "new_df = df.copy()\n",
        "# using the function to treat outliers\n",
        "new_df = clip_outliers(new_df)"
      ],
      "metadata": {
        "id": "T6ZOcbFFHipF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# checking the boxplot after outlier treatment\n",
        "\n",
        "# figsize\n",
        "plt.figure(figsize=(15,8))\n",
        "# title\n",
        "plt.suptitle('Outlier Analysis of Numerical Features', fontsize=20, fontweight='bold', y=1.02)\n",
        "\n",
        "for i,col in enumerate(num_features):\n",
        "  # subplot of 3 rows and 2 columns\n",
        "  plt.subplot(3, 2, i+1)\n",
        "\n",
        "  # countplot\n",
        "  sns.boxplot(new_df[col])\n",
        "  # x-axis label\n",
        "  plt.xlabel(col)\n",
        "  plt.tight_layout()"
      ],
      "metadata": {
        "id": "ikkzczBgILD7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# checking for distribution after treating outliers.\n",
        "\n",
        "# figsize\n",
        "plt.figure(figsize=(15,6))\n",
        "# title\n",
        "plt.suptitle('Data Distibution of Numerical Features', fontsize=20, fontweight='bold', y=1.02)\n",
        "\n",
        "for i,col in enumerate(num_features):\n",
        "  # subplots 3 rows, 2 columns\n",
        "  plt.subplot(3, 2, i+1)\n",
        "\n",
        "  # dist plots\n",
        "  sns.distplot(new_df[col])\n",
        "  # x-axis label\n",
        "  plt.xlabel(col)\n",
        "  plt.tight_layout()"
      ],
      "metadata": {
        "id": "FQ4tU4gdITHv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**We can also observe some shifts in the distribution of the data after treating outliers. Some of the data were skewed before handling outliers, but after doing so, the features almost follow the normal distribution. Therefore, we are not utilizing the numerical feature transformation technique.**"
      ],
      "metadata": {
        "id": "J3GW7PCZIbDV"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **5. Feature Engineering**"
      ],
      "metadata": {
        "id": "-lIVwmWWJGpX"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "1. Feature engineering is the process of transforming raw data into a set of meaningful, informative, and predictive features that can be used to train machine learning models.\n",
        "\n",
        "2. It involves selecting, creating, or modifying features in the dataset to enhance the performance and effectiveness of the models.\n",
        "\n",
        "3. Feature engineering is a critical step in machine learning because the quality and relevance of features can significantly impact the model's performance. Well-engineered features can help capture relevant patterns, relationships, and structures in the data, enabling the model to make accurate predictions or classifications"
      ],
      "metadata": {
        "id": "k3fLwu-fJPZ1"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **5.1 Linear relationship with target variable**"
      ],
      "metadata": {
        "id": "Gg58a9tw_JiP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Checking Linearity of all numerical features with our target variable\n",
        "\n",
        "# figsize\n",
        "plt.figure(figsize=(15, 10))\n",
        "\n",
        "# title\n",
        "plt.suptitle('Regression Analysis of Numerical features', fontsize=20, fontweight='bold', y=1.02)\n",
        "\n",
        "for i, col in enumerate(numerical_features):\n",
        "  if col==\"Rented Bike Count\":\n",
        "    pass\n",
        "  else:\n",
        "  # subplots of 3 rows and 3 columns\n",
        "    plt.subplot(3, 3, i+1)\n",
        "\n",
        "    # regression plots\n",
        "    sns.regplot(x= numerical_features[col], y = numerical_features['Rented Bike Count'], scatter_kws={\"color\": \"blue\"}, line_kws={\"color\": \"red\"})\n",
        "\n",
        "    plt.title(f'Dependend variable and {col}')\n",
        "    plt.tight_layout()\n"
      ],
      "metadata": {
        "id": "lCVka43U_HCo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Most of the features are positively correlated with target variable."
      ],
      "metadata": {
        "id": "zvdZejKW_1uc"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **5.2 Correlation heatmap**"
      ],
      "metadata": {
        "id": "PTV0VV9MAFpQ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "1. A correlation coefficient of 1 indicates a perfect positive linear relationship, where the variables increase or decrease together with a constant slope.\n",
        "2. A correlation coefficient of -1 indicates a perfect negative linear relationship, where the variables move in opposite directions with a constant slope.\n",
        "3. A correlation coefficient of 0 indicates no linear relationship between the variables.\n",
        "4. The correlation coefficient is calculated using the covariance between the variables divided by the product of their standard deviations.\n",
        "5. The correlation coefficient provides insight into the strength and direction of the relationship between variables.\n",
        "However, it only measures linear relationships and does not capture other types of associations, such as nonlinear or complex dependencies."
      ],
      "metadata": {
        "id": "DbxUgLzwARC1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Heatmap relative to all numeric columns\n",
        "corr_matrix = new_df.corr()\n",
        "mask = np.array(corr_matrix)\n",
        "mask[np.tril_indices_from(mask)] = False\n",
        "\n",
        "fig = plt.figure(figsize=(10, 10))\n",
        "sns.heatmap(corr_matrix, mask=mask, annot=True, cbar=True, vmax=0.8, vmin=-0.8, cmap='RdYlGn')\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "csKMCo8-AFU9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "plt.figure(figsize=(2,4), dpi=150)\n",
        "sns.heatmap(new_df.corr()[[\"Rented Bike Count\"]].sort_values\n",
        "            (by=\"Rented Bike Count\", ascending=False)[1:],annot=True)\n",
        "plt.title('Features Correlating with Rented Bike Count', fontsize=10, fontweight='bold', y=1.02);"
      ],
      "metadata": {
        "id": "0rqeDGlwBAfE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**From the above graph we could see that the columns Temperature and Dew Point Temperature are highly corelated. We can drop one of them. As the corelation between Temperature and our dependent variable \"Bike Rented Count\" is high compared to Dew Point Temperature. So we will Keep the Temperature column and drop the Dew Point Temperature column.**"
      ],
      "metadata": {
        "id": "TVcCkvEFA7CR"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# droping Dew point temperature column due to multi-collinearity\n",
        "\n",
        "new_df.drop('Dew point temperature', axis=1, inplace=True)\n"
      ],
      "metadata": {
        "id": "-Xxg7kHkBflN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **5.3 VIF**"
      ],
      "metadata": {
        "id": "n35zwZR9Bl8e"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "1. VIF, which stands for Variance Inflation Factor, is a measure used in regression analysis to assess multicollinearity among predictor variables.\n",
        "\n",
        "2. Multicollinearity occurs when predictor variables in a regression model are highly correlated with each other, which can cause issues in interpreting the individual effects of the variables and can lead to unstable and unreliable model estimates.\n",
        "\n",
        "3. The VIF quantifies the extent to which the variance of the estimated regression coefficient is inflated due to multicollinearity.\n",
        "\n",
        "4. It measures how much the variance of a particular predictor variable's estimated coefficient is increased compared to if that variable were uncorrelated with the other predictor variables in the model.\n",
        "\n",
        "**Interpreting VIF values:**\n",
        "\n",
        "1. A VIF of 1 indicates no multicollinearity, meaning the predictor variable is not correlated with the other predictors.\n",
        "2. A VIF greater than 1 suggests some degree of multicollinearity, where higher values indicate stronger correlation with other predictors.\n",
        "3. A commonly used threshold is a VIF value of 5 or 10. Variables with VIF values exceeding these thresholds are considered to have high multicollinearity and may need to be addressed.\n",
        "4. By examining VIF values, researchers can identify predictor variables that contribute to multicollinearity and take appropriate actions, such as removing highly correlated variables, combining variables, or gathering additional data to mitigate the multicollinearity issue."
      ],
      "metadata": {
        "id": "X6imQ_WXBr0v"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from statsmodels.stats.outliers_influence import variance_inflation_factor\n",
        "\n",
        "# function to calculate Multicollinearity\n",
        "\n",
        "def calculate_vif(X):\n",
        "\n",
        "  # For each X, calculate VIF and save in dataframe\n",
        "  vif = pd.DataFrame()\n",
        "  vif[\"VIF Factor\"] = [variance_inflation_factor(X.values, i) for i in range(X.shape[1])]\n",
        "  vif[\"features\"] = X.columns\n",
        "\n",
        "  return vif"
      ],
      "metadata": {
        "id": "IciRshqlBlmk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# multicollinearity result\n",
        "\n",
        "calculate_vif(new_df[[i for i in new_df.describe().columns if i not in ['Rented Bike Count','Date']]])"
      ],
      "metadata": {
        "id": "n8Du6z5FDdm6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **We are going to use these variables for further model building**"
      ],
      "metadata": {
        "id": "z_rpyotLDr6j"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **5.4 Encoding**"
      ],
      "metadata": {
        "id": "BZSN2quID0VI"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Encoding refers to the process of converting categorical variables into numerical representations that can be understood and processed by machine learning algorithms. Since many machine learning algorithms require numerical inputs, encoding categorical variables becomes necessary."
      ],
      "metadata": {
        "id": "xdLWgmhTMSyy"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# droping Year columns as it does not account for any information addition\n",
        "\n",
        "new_df.drop(['Year'], axis=1, inplace = True)\n",
        "categorical_features.drop('Year', axis = 1, inplace = True)"
      ],
      "metadata": {
        "id": "QD-aq8U-MUVT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Check Unique Values for each categorical variable.\n",
        "for i in categorical_features:\n",
        "  print(\"Number of unique values in\", i, \"is\" , new_df[i].nunique())"
      ],
      "metadata": {
        "id": "VY5N9sGIMdAi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "We will use one hot encoding for Seasons and Numeric encoding for Holiday and Functioning day. Other columns are already encoded."
      ],
      "metadata": {
        "id": "QksMTIUSO1vY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Creating a copy for checking\n",
        "Enc = new_df.copy()\n",
        "Enc =pd.get_dummies(Enc, columns=['Seasons'],prefix='Seasons',drop_first=True)"
      ],
      "metadata": {
        "id": "HcJnGfP-O9ah"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "Enc.head()"
      ],
      "metadata": {
        "id": "gPQ-Dvs3PKmU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "new_df.head()"
      ],
      "metadata": {
        "id": "F-3E3woCPT06"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Doing the same process on our data after checking the dummy function output\n",
        "\n",
        "new_df = pd.get_dummies(new_df, columns = ['Seasons'], prefix='Seasons', drop_first = True)"
      ],
      "metadata": {
        "id": "UuYuuVMrPoDj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "new_df.head()"
      ],
      "metadata": {
        "id": "teJfiVySPxuz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "new_df = pd.get_dummies(new_df, columns = ['Day'], prefix='Day', drop_first = True)"
      ],
      "metadata": {
        "id": "28uD8SIG2k6b"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "new_df"
      ],
      "metadata": {
        "id": "gSUuoWgk8t_5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "new_df = pd.get_dummies(new_df, columns = ['Month'], prefix='Month', drop_first = True)"
      ],
      "metadata": {
        "id": "Fnh9vRh-9unZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "new_df.head()"
      ],
      "metadata": {
        "id": "GbpW6yU09ztl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Numerical Encoding for holiday and functioning_day\n",
        "\n",
        "new_df['Holiday'] = new_df['Holiday'].map({'Holiday': 1, 'No Holiday': 0})\n",
        "\n",
        "new_df['Functioning Day'] = new_df['Functioning Day'].map({'Yes': 1, 'No': 0})"
      ],
      "metadata": {
        "id": "qgmAC_edP6z7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "new_df.head(2)"
      ],
      "metadata": {
        "id": "fj4wnbCgP8_p"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **5.5 Normalisation of target variable**"
      ],
      "metadata": {
        "id": "XAR6HBMUQLFd"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "fig, ax = plt.subplots(1,2 , figsize = (15,5))\n",
        "\n",
        "# Distribution plot of Rented Bike Count\n",
        "dist =sns.distplot(new_df['Rented Bike Count'],hist=True, ax = ax[0])\n",
        "dist.set(xlabel = 'Rented Bike Count', ylabel ='Density', title = 'Distribution Plot of Target Variable')\n",
        "\n",
        "# mean line\n",
        "dist.axvline(new_df['Rented Bike Count'].mean(), color='magenta', linestyle='dashed', linewidth=2)\n",
        "# median line\n",
        "dist.axvline(new_df['Rented Bike Count'].median(), color='black', linestyle='dashed', linewidth=2)\n",
        "\n",
        "# Boxplot\n",
        "box = sns.boxplot(new_df['Rented Bike Count'], ax= ax[1])\n",
        "box.set(title = 'Outlier Analysis of Target Variable')\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "q0mfAalqQQqr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Observation:**\n",
        "\n",
        "The graph above indicates that the Rented Bike Count has a moderate right skewness. Linear regression assumes that the dependent variable has a normal distribution, therefore, to meet this assumption, we need to take some measures to normalize the distribution.\n",
        "The boxplot above indicates that there are outliers in the rented bike count column.\n"
      ],
      "metadata": {
        "id": "t0nuGd1ZVNXN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#apply diffrent tranformation technique and checking data distributation\n",
        "fig,axes = plt.subplots(1,4,figsize=(20,5))\n",
        "sns.distplot((new_df['Rented Bike Count']),ax=axes[0],color='brown').set_title(\" Input data\");\n",
        "\n",
        "# here we use log10\n",
        "#transform only posible in positive value and >0 value so add 0.0000001 in data\n",
        "sns.distplot(np.log1p(new_df['Rented Bike Count']),ax=axes[1],color='red').set_title(\"log1p\");\n",
        "\n",
        "# here we use square root\n",
        "sns.distplot(np.sqrt(new_df['Rented Bike Count']),ax=axes[2], color='blue').set_title(\"Square root\");\n",
        "\n",
        "# here we use cube root\n",
        "sns.distplot(np.cbrt(new_df['Rented Bike Count']),ax=axes[3], color='green').set_title(\"cube root\");"
      ],
      "metadata": {
        "id": "wCT3X4NXUvL-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Observations:\n",
        "\n",
        "1. Applying a logarithmic transformation to the dependent variable did not help much as it resulted in a negatively skewed distribution.\n",
        "2. Cube root transformation was attempted, but it did not result in a normally distributed variable.\n",
        "3. Therefore, we will use a square root transformation for the regression as it transformed the variable into a well-distributed form."
      ],
      "metadata": {
        "id": "ce8fneOXVfuL"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "fig, ax = plt.subplots(1,2 , figsize = (15,5))\n",
        "\n",
        "#  checking square root tranformation in our target variable\n",
        "dist =sns.distplot(np.sqrt(new_df['Rented Bike Count']), ax = ax[0])\n",
        "dist.set(xlabel = 'Rented Bike Count', ylabel ='Density', title = 'Distribution Plot of Target Variable in sqrt tranformation')\n",
        "\n",
        "# mean line\n",
        "dist.axvline(np.sqrt(new_df['Rented Bike Count']).mean(), color='magenta', linestyle='dashed', linewidth=2)\n",
        "# median line\n",
        "dist.axvline(np.sqrt(new_df['Rented Bike Count']).median(), color='black', linestyle='dashed', linewidth=2)\n",
        "\n",
        "# Boxplot\n",
        "box = sns.boxplot(np.sqrt(new_df['Rented Bike Count']), ax= ax[1])\n",
        "box.set(title = 'Outlier Analysis of Target Variable in sqrt tranformation')\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "Em54YnO1Vs80"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Observation**-\n",
        "\n",
        "1. By applying the square root transformation to the skewed Rented Bike Count, we were able to obtain an almost normal distribution, which is in line with the general rule that skewed variables should be normalized in linear regression.\n",
        "2. We found that there are no outliers in the Rented Bike Count column after applying square root transformation."
      ],
      "metadata": {
        "id": "crVt3dK7XViE"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# applying square root on Rented_Bike_Count\n",
        "new_df['Rented Bike Count']=np.sqrt(new_df['Rented Bike Count'])"
      ],
      "metadata": {
        "id": "F5O7WjKvXWEu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Manipulations done and insights found**"
      ],
      "metadata": {
        "id": "fu-U3p5gbx8k"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "1. We checked for correlation coefficient and found that most of the numerical features are positively correlated to our target variable.\n",
        "2. From heatmap and correlation coefficient, dew_point_temperature and temperature have a correlation coefficient of 0.91 and dew_point_temperature is less correlated to our target variable hence we dropped dew_point_temperature.\n",
        "3. We also did a VIF analysis to remove multi-colinearity and since the VIF factor of 'year' is too large hence we removed the year from our data to build our model.\n",
        "We encoded our categorical features which are necessary for the model to understand. We used one hot encoding for 'seasons' and Numeric encoding for 'holiday' and 'functioning_day'. Other columns are already encoded.\n",
        "4. To treat our target variable we Applied a square root transformation for the regression as it transformed the target variable into a well-distributed form."
      ],
      "metadata": {
        "id": "9FZ8fyU1XgKV"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **6. Model Building**"
      ],
      "metadata": {
        "id": "8FbPbQx-cEqV"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **6.1 Train Test Split**"
      ],
      "metadata": {
        "id": "CVhQtPRBcLk2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#X = independent variable and y = target variable\n",
        "X = new_df.drop('Rented Bike Count', axis=1)\n",
        "y= new_df['Rented Bike Count']"
      ],
      "metadata": {
        "id": "FHol3u0KYGGf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# train test split\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=33)\n",
        "\n",
        "print(X_train.shape)\n",
        "print(X_test.shape)"
      ],
      "metadata": {
        "id": "6yHqSj1gYIcE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **6.2 Scaling Data**"
      ],
      "metadata": {
        "id": "SHTILNQGeJ9J"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Scaling Data\n",
        "scaler = StandardScaler()\n",
        "\n",
        "X_train = scaler.fit_transform(X_train)\n",
        "X_test = scaler.transform(X_test)"
      ],
      "metadata": {
        "id": "1bWnsA-sYylD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **6.3 Model Training**"
      ],
      "metadata": {
        "id": "LDCJptgr5eS2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# empty list for appending performance metric score\n",
        "model_result = []\n",
        "\n",
        "def predict(ml_model,model_name):\n",
        "\n",
        "  '''\n",
        "  Pass the model and predict value.\n",
        "  Function will calculate all the evaluation metrics and appending those metrics score on model_result list.\n",
        "  Plotting different graphs for test data.\n",
        "  '''\n",
        "\n",
        "  # model fitting\n",
        "  model = ml_model.fit(X_train,y_train)\n",
        "\n",
        "  # predicting values\n",
        "  y_train_pred = model.predict(X_train)\n",
        "  y_test_pred = model.predict(X_test)\n",
        "\n",
        "  # Reverse the transformation on the predictions    (In case if we need y_train_pred in original and transformed way)\n",
        "  y_train_pred_original = np.power(y_train_pred, 2)\n",
        "  y_test_pred_original = np.power(y_test_pred, 2)\n",
        "\n",
        "  # graph --> best fit line on test data\n",
        "  sns.regplot(x=y_test_pred, y=y_test, line_kws={'color':'red'})\n",
        "  plt.xlabel('Predicted')\n",
        "  plt.ylabel('Actual')\n",
        "\n",
        "  '''Evaluation metrics on train data'''\n",
        "  train_MSE  = round(mean_squared_error(y_train, y_train_pred),3)\n",
        "  train_RMSE = round(np.sqrt(train_MSE),3)\n",
        "  train_r2 = round(r2_score(y_train, y_train_pred),3)\n",
        "  train_MAE = round(mean_absolute_error(y_train, y_train_pred),3)\n",
        "  train_adj_r2 = round(1-(1-r2_score(y_train, y_train_pred))*((X_train.shape[0]-1)/(X_train.shape[0]-X_train.shape[1]-1)),3)\n",
        "  print(f'train MSE : {train_MSE}')\n",
        "  print(f'train RMSE : {train_RMSE}')\n",
        "  print(f'train MAE : {train_MAE}')\n",
        "  print(f'train R2 : {train_r2}')\n",
        "  print(f'train Adj R2 : {train_adj_r2}')\n",
        "  print('-'*150)\n",
        "\n",
        "  '''Evaluation metrics on test data'''\n",
        "  test_MSE  = round(mean_squared_error(y_test, y_test_pred),3)\n",
        "  test_RMSE = round(np.sqrt(test_MSE),3)\n",
        "  test_r2 = round(r2_score(y_test, y_test_pred),3)\n",
        "  test_MAE = round(mean_absolute_error(y_test, y_test_pred),3)\n",
        "  test_adj_r2 = round(1-(1-r2_score(y_test, y_test_pred))*((X_test.shape[0]-1)/(X_test.shape[0]-X_test.shape[1]-1)),3)\n",
        "  print(f'test MSE : {test_MSE}')\n",
        "  print(f'test RMSE : {test_RMSE}')\n",
        "  print(f'test MAE : {test_MAE}')\n",
        "  print(f'test R2 : {test_r2}')\n",
        "  print(f'test Adj R2 : {test_adj_r2}')\n",
        "  print('-'*150)\n",
        "\n",
        "  # graph --> actual vs predicted on test data\n",
        "  plt.figure(figsize=(6,5))\n",
        "  plt.plot((y_test_pred)[:20])\n",
        "  plt.plot(np.array((y_test)[:20]))\n",
        "  plt.legend([\"Predicted\",\"Actual\"])\n",
        "  plt.xlabel('Test Data on last 20 points')\n",
        "  plt.show()\n",
        "  print('-'*150)\n",
        "\n",
        "  '''actual vs predicted value on test data'''\n",
        "  d = {'y_actual':y_test, 'y_predict':y_test_pred, 'error':y_test-y_test_pred}\n",
        "  print(pd.DataFrame(data=d).head().T)\n",
        "  print('-'*150)\n",
        "\n",
        "  # using the score from the performance metrics to create the final model_result.\n",
        "  model_result.append({'model':model_name,\n",
        "                       'train MSE':train_MSE,\n",
        "                       'test MSE':test_MSE,\n",
        "                       'train RMSE':train_RMSE,\n",
        "                       'test RMSE':test_RMSE,\n",
        "                       'train MAE':train_MAE,\n",
        "                       'test MAE':test_MAE,\n",
        "                       'train R2':train_r2,\n",
        "                       'test R2':test_r2,\n",
        "                       'train Adj R2':train_adj_r2,\n",
        "                       'test Adj R2':test_adj_r2})"
      ],
      "metadata": {
        "id": "uMYU5L1Xc5Z9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **7. Model Implementation**"
      ],
      "metadata": {
        "id": "dn4sxNcl6K9A"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **7.1 Linear Regression**"
      ],
      "metadata": {
        "id": "pnP8mbfA6SCl"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Linear regression is a statistical method used to model the relationship between a dependent variable and one or more independent variables by fitting a linear equation to the observed data. The goal is to find the best-fitting line that can predict the value of the dependent variable based on the values of the independent variables."
      ],
      "metadata": {
        "id": "KCjZtQzq6q-x"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "predict(LinearRegression(), 'LinearRegression')"
      ],
      "metadata": {
        "id": "MjswAy1a6RQB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **8.2 Lasso**"
      ],
      "metadata": {
        "id": "zdVMsvWk8xAK"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Lasso (Least Absolute Shrinkage and Selection Operator) is a regularization technique used in linear regression models. It helps to reduce the complexity of the model and improve its generalization ability by penalizing the magnitude of coefficients of the features.\n",
        "\n",
        "The lasso regularization adds a penalty term to the loss function being optimized. The penalty term is proportional to the absolute magnitude of the coefficients, but unlike ridge regression, it shrinks the coefficients of some features to zero, effectively removing them from the model."
      ],
      "metadata": {
        "id": "a21nXhck83HW"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "predict(Lasso(alpha=0.1, max_iter=1000), 'Lasso')"
      ],
      "metadata": {
        "id": "tq012Uuzc5Xi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **8.3 Ridge**"
      ],
      "metadata": {
        "id": "Rq64Y6Yz9JaF"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Ridge Regression is a type of regularized linear regression that aims to solve the problem of multicollinearity and overfitting by adding a penalty term to the loss function. The penalty term is the L2 regularization term (also known as the weight decay term), which adds a penalty proportional to the square of the magnitude of the coefficients."
      ],
      "metadata": {
        "id": "ybVzwfQI9P4c"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "predict(Ridge(alpha=0.1, max_iter=1000), 'Ridge')"
      ],
      "metadata": {
        "id": "7DId2Ltpc5Ux"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **8.4 Elastic Net**"
      ],
      "metadata": {
        "id": "PP3m08-_9YMd"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "ElasticNet is a linear regression algorithm that combines both L1 (Lasso) and L2 (Ridge) regularization techniques. L1 and L2 regularization are methods used to prevent overfitting by adding penalty terms to the loss function that the algorithm minimizes. Lasso adds a penalty proportional to the absolute value of the coefficients, while Ridge adds a penalty proportional to the square of the coefficients."
      ],
      "metadata": {
        "id": "ZgZ1I3ED9yuR"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "predict(ElasticNet(alpha=0.1, max_iter=1000), 'Elastic Net')"
      ],
      "metadata": {
        "id": "HtIM6Ay59XEV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **8.5 K-Nearest Neighbors**"
      ],
      "metadata": {
        "id": "LpEUEtBW_mZZ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "A supervised machine learning algorithm known as KNN or K-nearest neighbor can be used to solve classification and regression problems. K is not a non-parametric nearest neighbor, i.e. It makes no assumptions regarding the assumptions that underlie the data. An input or unseen data set is categorized here by the algorithm based on the characteristics shared by the closest data points. The distance between two points determines these closest neighbors. The distance metric methods that are utilized can be Euclidean Distance, Manhattan Distance, Minkowski, Cosine Similarity Measure etc)"
      ],
      "metadata": {
        "id": "IZz9TEdg_rI8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "predict(KNeighborsRegressor(n_neighbors=3),'KNN')"
      ],
      "metadata": {
        "id": "yekI7is3_N9d"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **8.6 Support Vector Machine**"
      ],
      "metadata": {
        "id": "METLOZZZ_ygm"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Support Vector Machine (SVM) is a popular and powerful machine learning algorithm for classification and regression problems. It is based on the concept of finding the best hyperplane that separates the data into classes, or predicts the target value for regression problems."
      ],
      "metadata": {
        "id": "hD44Mim-_6SB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "predict(SVR(kernel='rbf',C=100), 'SVM')"
      ],
      "metadata": {
        "id": "slUQ8vqrc5SE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **8.7 Decision Tree**"
      ],
      "metadata": {
        "id": "v_MBUmxaACyi"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "A decision tree is a tree-like model used in machine learning to make predictions or decisions by breaking down a set of rules or conditions into smaller and smaller sub-conditions, based on the values of the input features.\n",
        "\n",
        "Each node in the tree represents a test on a feature, and each branch represents the outcome of the test. The final branches of the tree, called the leaves, represent the class predictions or decisions. The tree is built recursively by finding the best feature to split the data based on the information gain or decrease in impurity at each node."
      ],
      "metadata": {
        "id": "4J5DDCKlAH5x"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "predict(DecisionTreeRegressor(min_samples_leaf=20, min_samples_split=3,max_depth=20, random_state=33), 'Decision Tree')"
      ],
      "metadata": {
        "id": "L3CWHqL9AFfC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **8.8 Random Forest**"
      ],
      "metadata": {
        "id": "I3dOXiuBAzOn"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Random Forest is an ensemble machine learning algorithm that builds multiple decision trees and combines their predictions to make a final classification or regression prediction. In contrast to a single decision tree, Random Forest reduces the risk of overfitting by combining the results of many trees, each built on a different subset of the data."
      ],
      "metadata": {
        "id": "iQg7WHQxA3oA"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Hyperparameter Tunning using GridSearchCV**"
      ],
      "metadata": {
        "id": "OUVnxJDRA7hp"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "param_grid = {'n_estimators': [50,80],       # number of trees in the ensemble\n",
        "             'max_depth': [15,20],           # maximum number of levels allowed in each tree.\n",
        "             'min_samples_split': [5,15],    # minimum number of samples necessary in a node to cause node splitting.\n",
        "             'min_samples_leaf': [3,5]}      # minimum number of samples which can be stored in a tree leaf.\n",
        "\n",
        "\n",
        "# Initialize the RandomForestRegressor model\n",
        "rf = RandomForestRegressor()\n",
        "\n",
        "# Use GridSearchCV to perform a grid search over the parameter grid\n",
        "grid_search = GridSearchCV(rf, param_grid=param_grid, cv=5, scoring='r2')\n",
        "\n",
        "# Fit the model to the training data\n",
        "grid_search.fit(X, y)"
      ],
      "metadata": {
        "id": "a2AT2sGYA1fv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Get the best parameters from the grid search\n",
        "rf_optimal_model = grid_search.best_estimator_\n",
        "rf_optimal_model"
      ],
      "metadata": {
        "id": "IAp4qPb_BEPY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "predict(rf_optimal_model, 'Random Forest')"
      ],
      "metadata": {
        "id": "sKRt8UFqBIBM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Model Explainability**"
      ],
      "metadata": {
        "id": "43bLIfFEBM9T"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# feature importance\n",
        "importances = rf_optimal_model.feature_importances_\n",
        "\n",
        "# Creating a dictonary\n",
        "importance_dict = {'Feature' : list(X.columns),\n",
        "                   'Feature Importance' : importances}\n",
        "\n",
        "# Creating the dataframe\n",
        "importance = pd.DataFrame(importance_dict)\n",
        "sorting_features = importance.sort_values(by=['Feature Importance'],ascending=False)\n",
        "sorting_features"
      ],
      "metadata": {
        "id": "UNNjFPglBO9E"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# plotting feature importance graph\n",
        "plt.figure(figsize=(15,5))\n",
        "bar = sns.barplot(x='Feature Importance', y='Feature', data=sorting_features, color='blue')\n",
        "bar.set_title('Important Features')\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "lH1VJqwlBWgQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**The top 5 important features in Random Forest are temperature, hour, functioning_day, rainfall, and humidity**"
      ],
      "metadata": {
        "id": "1lnJr_-sBZzN"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **8.9 AdaBoost**"
      ],
      "metadata": {
        "id": "qdNkHJbCBgXw"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "AdaBoost (Adaptive Boosting) is an ensemble machine learning algorithm that combines multiple weak models to form a stronger model. It works by assigning weights to the data points in a dataset and iteratively building weak models that try to correctly classify or predict the target variable. After each iteration, the weights of the misclassified or mispredicted data points are increased, making it more likely that the next weak model will focus on these points."
      ],
      "metadata": {
        "id": "IIwZgJMcBl5Z"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Create a base decision tree regression model\n",
        "dt = DecisionTreeRegressor(max_depth=12)\n",
        "\n",
        "# Initialize the AdaBoost regression model\n",
        "ada = AdaBoostRegressor(base_estimator=dt, n_estimators=60, learning_rate=1, random_state =33)\n",
        "\n",
        "# Predict using function\n",
        "predict(ada, 'AdaBoost')"
      ],
      "metadata": {
        "id": "O9KRXFWzBbTE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "new_df.info()"
      ],
      "metadata": {
        "id": "XjpLRJwCJ7TV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "new_df[\"Hour\"]"
      ],
      "metadata": {
        "id": "r-NaNr7VKFjo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "new_df['Hour'] = new_df['Hour'].astype('int')"
      ],
      "metadata": {
        "id": "YAJ8q6j_Kc1t"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "new_df.info()"
      ],
      "metadata": {
        "id": "oiXZWlLsKsco"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "new_df[\"Functioning Day\"]"
      ],
      "metadata": {
        "id": "UQjOQ3j1Ngf_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Test XGboost**"
      ],
      "metadata": {
        "id": "H5RcKb06Gpcv"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "X1 = Txg.drop('Rented Bike Count', axis=1)\n",
        "y1= Txg['Rented Bike Count']"
      ],
      "metadata": {
        "id": "93nc95wgN_OZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# train test split\n",
        "X_train, X_test, y_train, y_test = train_test_split(X1, y1, test_size=0.2, random_state=33)\n",
        "\n",
        "print(X_train.shape)\n",
        "print(X_test.shape)"
      ],
      "metadata": {
        "id": "yxrG6Of-N_1z"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# empty list for appending performance metric score\n",
        "model_result = []\n",
        "\n",
        "def predict(ml_model,model_name):\n",
        "\n",
        "  '''\n",
        "  Pass the model and predict value.\n",
        "  Function will calculate all the evaluation metrics and appending those metrics score on model_result list.\n",
        "  Plotting different graphs for test data.\n",
        "  '''\n",
        "\n",
        "  # model fitting\n",
        "  model = ml_model.fit(X_train,y_train)\n",
        "\n",
        "  # predicting values\n",
        "  y_train_pred = model.predict(X_train)\n",
        "  y_test_pred = model.predict(X_test)\n",
        "\n",
        "  # Reverse the transformation on the predictions    (In case if we need y_train_pred in original and transformed way)\n",
        "  y_train_pred_original = np.power(y_train_pred, 2)\n",
        "  y_test_pred_original = np.power(y_test_pred, 2)\n",
        "\n",
        "  # graph --> best fit line on test data\n",
        "  sns.regplot(x=y_test_pred, y=y_test, line_kws={'color':'red'})\n",
        "  plt.xlabel('Predicted')\n",
        "  plt.ylabel('Actual')\n",
        "\n",
        "  '''Evaluation metrics on train data'''\n",
        "  train_MSE  = round(mean_squared_error(y_train, y_train_pred),3)\n",
        "  train_RMSE = round(np.sqrt(train_MSE),3)\n",
        "  train_r2 = round(r2_score(y_train, y_train_pred),3)\n",
        "  train_MAE = round(mean_absolute_error(y_train, y_train_pred),3)\n",
        "  train_adj_r2 = round(1-(1-r2_score(y_train, y_train_pred))*((X_train.shape[0]-1)/(X_train.shape[0]-X_train.shape[1]-1)),3)\n",
        "  print(f'train MSE : {train_MSE}')\n",
        "  print(f'train RMSE : {train_RMSE}')\n",
        "  print(f'train MAE : {train_MAE}')\n",
        "  print(f'train R2 : {train_r2}')\n",
        "  print(f'train Adj R2 : {train_adj_r2}')\n",
        "  print('-'*150)\n",
        "\n",
        "  '''Evaluation metrics on test data'''\n",
        "  test_MSE  = round(mean_squared_error(y_test, y_test_pred),3)\n",
        "  test_RMSE = round(np.sqrt(test_MSE),3)\n",
        "  test_r2 = round(r2_score(y_test, y_test_pred),3)\n",
        "  test_MAE = round(mean_absolute_error(y_test, y_test_pred),3)\n",
        "  test_adj_r2 = round(1-(1-r2_score(y_test, y_test_pred))*((X_test.shape[0]-1)/(X_test.shape[0]-X_test.shape[1]-1)),3)\n",
        "  print(f'test MSE : {test_MSE}')\n",
        "  print(f'test RMSE : {test_RMSE}')\n",
        "  print(f'test MAE : {test_MAE}')\n",
        "  print(f'test R2 : {test_r2}')\n",
        "  print(f'test Adj R2 : {test_adj_r2}')\n",
        "  print('-'*150)\n",
        "\n",
        "  # graph --> actual vs predicted on test data\n",
        "  plt.figure(figsize=(6,5))\n",
        "  plt.plot((y_test_pred)[:20])\n",
        "  plt.plot(np.array((y_test)[:20]))\n",
        "  plt.legend([\"Predicted\",\"Actual\"])\n",
        "  plt.xlabel('Test Data on last 20 points')\n",
        "  plt.show()\n",
        "  print('-'*150)\n",
        "\n",
        "  '''actual vs predicted value on test data'''\n",
        "  d = {'y_actual':y_test, 'y_predict':y_test_pred, 'error':y_test-y_test_pred}\n",
        "  print(pd.DataFrame(data=d).head().T)\n",
        "  print('-'*150)\n",
        "\n",
        "  # using the score from the performance metrics to create the final model_result.\n",
        "  model_result.append({'model':model_name,\n",
        "                       'train MSE':train_MSE,\n",
        "                       'test MSE':test_MSE,\n",
        "                       'train RMSE':train_RMSE,\n",
        "                       'test RMSE':test_RMSE,\n",
        "                       'train MAE':train_MAE,\n",
        "                       'test MAE':test_MAE,\n",
        "                       'train R2':train_r2,\n",
        "                       'test R2':test_r2,\n",
        "                       'train Adj R2':train_adj_r2,\n",
        "                       'test Adj R2':test_adj_r2})"
      ],
      "metadata": {
        "id": "cQzTD-uvOFVi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "Txg=new_df.copy()"
      ],
      "metadata": {
        "id": "DClWhbmFGoX3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "Txg.info()"
      ],
      "metadata": {
        "id": "87TsSFiJJOCO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "Txg[\"Hour\"].astype(\"int\")"
      ],
      "metadata": {
        "id": "-e9TITfwLwWb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "Txg[['Holiday', 'Functioning Day',\"Seasons_Spring\",\"Seasons_Summer\",\"Seasons_Winter\",\"Day_Monday\",\"Day_Saturday\",\"Day_Sunday\",\"Day_Thursday\",\"Day_Tuesday\",\"Day_Wednesday\",\"Month_August\",\"Month_December\",\"Month_February\",\"Month_January\",\"Month_July\",\"Month_June\",\"Month_March\",\"Month_May\",\"Month_November\",\"Month_October\",\"Month_September\"]] = Txg[['Holiday', 'Functioning Day',\"Seasons_Spring\",\"Seasons_Summer\",\"Seasons_Winter\",\"Day_Monday\",\"Day_Saturday\",\"Day_Sunday\",\"Day_Thursday\",\"Day_Tuesday\",\"Day_Wednesday\",\"Month_August\",\"Month_December\",\"Month_February\",\"Month_January\",\"Month_July\",\"Month_June\",\"Month_March\",\"Month_May\",\"Month_November\",\"Month_October\",\"Month_September\"]].astype(\"category\")"
      ],
      "metadata": {
        "id": "nwgFSljTNeJn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "Txg.info()"
      ],
      "metadata": {
        "id": "iKQ6jaGANogY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "param_grid = {'n_estimators': [300,500],     # number of trees in the ensemble\n",
        "             'max_depth': [7,8],             # maximum number of levels allowed in each tree.\n",
        "             'min_samples_split': [3,5],     # minimum number of samples necessary in a node to cause node splitting.\n",
        "             'min_samples_leaf': [3,5]}      # minimum number of samples which can be stored in a tree leaf.\n",
        "\n",
        "\n",
        "# Initialize the RandomForestRegressor model\n",
        "xgb = XGBRegressor()\n",
        "Xy = xgb.DMatrix(X1, y1, enable_categorical=True)\n",
        "\n",
        "# Use GridSearchCV to perform a grid search over the parameter grid\n",
        "grid_search = GridSearchCV(Xy, param_grid=param_grid, cv=5, scoring='r2')\n",
        "\n",
        "# Fit the model to the training data\n",
        "grid_search.fit(X1, y1)"
      ],
      "metadata": {
        "id": "0q1hCbXXNtS5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **8.10 Xtreme Gradient Boosting**"
      ],
      "metadata": {
        "id": "nwD-oHObBs_q"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "XGBoost (eXtreme Gradient Boosting) is an optimized implementation of the Gradient Boosting algorithm that is specifically designed for large-scale and complex data. XGBoost is an ensemble learning algorithm that builds multiple decision trees and combines their predictions to make a final prediction."
      ],
      "metadata": {
        "id": "8PuJmWnTBxHG"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**HyperParameter Tunning using GridSearchCV**"
      ],
      "metadata": {
        "id": "M2fYL7iAB0IL"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "param_grid = {'n_estimators': [300,500],     # number of trees in the ensemble\n",
        "             'max_depth': [7,8],             # maximum number of levels allowed in each tree.\n",
        "             'min_samples_split': [3,5],     # minimum number of samples necessary in a node to cause node splitting.\n",
        "             'min_samples_leaf': [3,5]}      # minimum number of samples which can be stored in a tree leaf.\n",
        "\n",
        "\n",
        "# Initialize the RandomForestRegressor model\n",
        "xgb = XGBRegressor()\n",
        "\n",
        "# Use GridSearchCV to perform a grid search over the parameter grid\n",
        "grid_search = GridSearchCV(xgb, param_grid=param_grid, cv=5, scoring='r2')\n",
        "\n",
        "# Fit the model to the training data\n",
        "grid_search.fit(X, y)"
      ],
      "metadata": {
        "id": "JkdmPcJJBu7k"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Get the best parameters from the grid search\n",
        "xgb_optimal_model = grid_search.best_estimator_\n",
        "xgb_optimal_model"
      ],
      "metadata": {
        "id": "Xmrd_KMgB6xS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "predict(xgb_optimal_model, 'XGB')"
      ],
      "metadata": {
        "id": "Rub2UdtxB9Ry"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Model Explainability**"
      ],
      "metadata": {
        "id": "LD4pqTHlCAnH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# feature importance\n",
        "importances = xgb_optimal_model.feature_importances_\n",
        "\n",
        "# Creating a dictonary\n",
        "importance_dict = {'Feature' : list(X.columns),\n",
        "                   'Feature Importance' : importances}\n",
        "\n",
        "# Creating the dataframe\n",
        "importance = pd.DataFrame(importance_dict)\n",
        "sorting_features = importance.sort_values(by=['Feature Importance'],ascending=False)\n",
        "sorting_features"
      ],
      "metadata": {
        "id": "7cRyA0SrCBx_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# plotting feature importance graph\n",
        "plt.figure(figsize=(15,5))\n",
        "bar = sns.barplot(x='Feature Importance', y='Feature', data=sorting_features, color='blue')\n",
        "bar.set_title('Important Features')\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "OyVEcbA5CGqt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**The top 5 important features in XGB Model are season_winter, functioning_day, rainfall, hour, and season_autumn**"
      ],
      "metadata": {
        "id": "LXb1xCLiCKNw"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **8.11 Light GBM**"
      ],
      "metadata": {
        "id": "Q-VBgtNzCOSz"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "LightGBM is a gradient boosting framework that uses tree-based learning algorithms. It is designed to be more efficient than traditional gradient boosting algorithms and is particularly well-suited for large datasets.LightGBM is an open-source library that was developed by Microsoft.\n",
        "\n",
        "One of the key features of LightGBM is its use of a histogram-based approach to split nodes in decision trees."
      ],
      "metadata": {
        "id": "eIs2nnPZCS0m"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**HyperParameter Tunning using GridSearchCV**"
      ],
      "metadata": {
        "id": "UQgFemAFCVmW"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "param_grid = {'n_estimators': [600,800],     # number of trees in the ensemble\n",
        "             'max_depth': [8,10],            # maximum number of levels allowed in each tree.\n",
        "             'min_samples_split': [3,5],     # minimum number of samples necessary in a node to cause node splitting.\n",
        "             'min_samples_leaf': [2,3]}      # minimum number of samples which can be stored in a tree leaf.\n",
        "\n",
        "\n",
        "# Initialize the RandomForestRegressor model\n",
        "lgb = LGBMRegressor()\n",
        "\n",
        "# Use GridSearchCV to perform a grid search over the parameter grid\n",
        "grid_search = GridSearchCV(lgb, param_grid=param_grid, cv=5, scoring='r2')\n",
        "\n",
        "# Fit the model to the training data\n",
        "grid_search.fit(X_train, y_train)\n"
      ],
      "metadata": {
        "id": "7ILrePPeCMIP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Get the best parameters from the grid search\n",
        "lgb_optimal_model = grid_search.best_estimator_\n",
        "lgb_optimal_model"
      ],
      "metadata": {
        "id": "ehoc_BIfCcSM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "predict(lgb_optimal_model, 'LGB')"
      ],
      "metadata": {
        "id": "GJMnqY2gCe4q"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Model Explainability**"
      ],
      "metadata": {
        "id": "GLyuMZ2WCkhR"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# feature importance\n",
        "importances = lgb_optimal_model.feature_importances_\n",
        "\n",
        "#Creating a dictonary\n",
        "importance_dict = {'Feature' : list(X.columns),\n",
        "                   'Feature Importance' : importances}\n",
        "\n",
        "#Creating the dataframe\n",
        "importance = pd.DataFrame(importance_dict)\n",
        "sorting_features = importance.sort_values(by=['Feature Importance'],ascending=False)\n",
        "sorting_features"
      ],
      "metadata": {
        "id": "6xHJaajuCmMB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# plotting feature importance graph\n",
        "plt.figure(figsize=(15,5))\n",
        "bar=sns.barplot(x='Feature Importance', y='Feature', data=sorting_features, color='blue')\n",
        "bar.set_title('Important Features')\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "azlwFgviCtI9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**The top 5 important features in XGB Model are temperature, humidity, visibility, hour, and day.**"
      ],
      "metadata": {
        "id": "IPDCr8gCCxHD"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **8.12 Model Result**"
      ],
      "metadata": {
        "id": "wHmo3jB2C9KE"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The fit of the model to the dependent variables can be evaluated using the R square measure. On the other hand, overfitting is not taken into consideration. If there are a lot of independent variables in the regression model, it may work well with training data but fail with testing data because it is too complicated. Adjusted R Square is a new metric that penalizes additional independent variables added to the model and adjusts the metric to prevent overfitting.\n",
        "\n",
        "Because it estimates the relationship between the movements of a dependent variable and those of an independent variable, R square is the best evaluation method for predicting the rented_bike_count."
      ],
      "metadata": {
        "id": "QqKs0HqYDC6A"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# converting the model_result list into DataFrame\n",
        "model_result = pd.DataFrame(model_result)\n",
        "\n",
        "# sorting the values by test R2 score\n",
        "model_result.sort_values(by='test R2', ascending=False)"
      ],
      "metadata": {
        "id": "PuNdxnyZCy4T"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# plotting graph to compare model performance of all the models\n",
        "fig, ax = plt.subplots(1,2, figsize=(20,5))\n",
        "sns.barplot(x=model_result['model'], y=model_result['test R2'], ax=ax[0])           # Model Vs test R2\n",
        "sns.barplot(x=model_result['model'], y=model_result['test Adj R2'], ax=ax[1])       # Model Vs test Adj R2\n",
        "plt.tight_layout()"
      ],
      "metadata": {
        "id": "5lKICe4dDJxF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**From the above result, we can select XGB or LGB Regressor as the final model because it has the lowest RMSE value as well as the highest R2 score on the test data. I would go with LGB because it is considered a better model for large dataset and it is explaining the features which it is taking into account in better manner.**"
      ],
      "metadata": {
        "id": "2ci1KUmSDNAn"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **9. Conclusion**"
      ],
      "metadata": {
        "id": "4FiMmDjkDR8G"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Summary**"
      ],
      "metadata": {
        "id": "Eg56GlK1DWk5"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "We began our analysis by performing EDA on all of our datasets. First, we looked at and changed our dependent variable, \"Rental Bike Count.\" After that, we looked at categorical variables and numerical variables and discovered their correlation, distribution, and connection to the dependent variable. Additionally, we hot-encoded the categorical variables and removed some numerical features which are used only for EDA purposes and have multi-collinearity.\n",
        "\n",
        "Following that, we examine several well-known individual models, ranging from straightforward Linear Regression and Regularization Models (Ridge, Lasso, and Elastic Net) to more complex and ensemble models like Random Forest, Gradient Boost, and Light GBM. To enhance the performance of our model, we performed hyperparameter tuning."
      ],
      "metadata": {
        "id": "DJ7BFucPDbJO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Conclusion**"
      ],
      "metadata": {
        "id": "SEEYAe52DeTe"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "1. Here are some solutions to manage Bike Sharing Demand.\n",
        "- The majority of rentals are for daily commutes to workplaces and colleges. Therefore open additional stations near these landmarks to reach their primary customers.\n",
        "- While planning for extra bikes to stations the peak rental hours must be considered, i.e. 7–9 am and 5–6 pm.\n",
        "- Maintenance activities for bikes should be done at night due to the low usage of bikes during the night time. Removing some bikes from the streets at night time will not cause trouble for the customers.\n",
        "2. We see 2 rental patterns across the day in bike rental count - first for a Working Day where the rental count is high at peak office hours (8 am and 5 pm) and the second for a Non-working day where the rental count is more or less uniform across the day with a peak at around noon.\n",
        "3. Hour of the day: Bike rental count is mostly correlated with the time of the day. As indicated above, the count reaches a high point during peak hours on a working day and is mostly uniform during the day on a non-working day.\n",
        "4. Temperature: People generally prefer to bike at moderate to high temperatures. We see the highest rental counts between 32 to 36 degrees Celcius\n",
        "5. Season: We see the highest number of bike rentals in the Spring (July to September) and Summer (April to June) Seasons and the lowest in the Winter (January to March) season.\n",
        "6. Weather: As one would expect, we see the highest number of bike rentals on a clear day and the lowest on a snowy or rainy day\n",
        "7. Humidity: With increasing humidity, we see a decrease in the bike rental count.\n",
        "8. I have chosen the Light GBM model which is above all else I want better expectations for the rented_bike_count and time isn't compelling here. As a result, various linear models, decision trees, Random Forests, and Gradient Boost techniques were used to improve accuracy. I compared R2 metrics to choose a model.\n",
        "9. Due to less no. of data in the dataset, the training R2 score is around 99% and the test R2 score is 92.5%. Once we get more data we can retrain our algorithm for better performance."
      ],
      "metadata": {
        "id": "0qz4ghNJDxah"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Way Forward**"
      ],
      "metadata": {
        "id": "44O036lHEuV3"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "However, this is not the ultimate end. As this data is time-dependent, the values for variables like temperature, wind speed, solar radiation, etc., will not always be consistent. Therefore, there will be scenarios where the model might not perform well. As Machine learning is an exponentially evolving field, we will have to be prepared for all contingencies and also keep checking our model from time to time. Therefore, having quality knowledge and keeping pace with the ever-evolving ML field would surely help one to stay a step ahead in the future."
      ],
      "metadata": {
        "id": "iGspxcZZEzEh"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### ***Hurrah! You have successfully completed your Machine Learning Capstone Project !!!***"
      ],
      "metadata": {
        "id": "yM0DFvaHEczR"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "APOE70LHDOfE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "MEfZhC3rU_kM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## ***5. Hypothesis Testing***"
      ],
      "metadata": {
        "id": "g-ATYxFrGrvw"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Based on your chart experiments, define three hypothetical statements from the dataset. In the next three questions, perform hypothesis testing to obtain final conclusion about the statements through your code and statistical testing."
      ],
      "metadata": {
        "id": "Yfr_Vlr8HBkt"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Answer Here."
      ],
      "metadata": {
        "id": "-7MS06SUHkB-"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Hypothetical Statement - 1"
      ],
      "metadata": {
        "id": "8yEUt7NnHlrM"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 1. State Your research hypothesis as a null hypothesis and alternate hypothesis."
      ],
      "metadata": {
        "id": "tEA2Xm5dHt1r"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Answer Here."
      ],
      "metadata": {
        "id": "HI9ZP0laH0D-"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 2. Perform an appropriate statistical test."
      ],
      "metadata": {
        "id": "I79__PHVH19G"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Perform Statistical Test to obtain P-Value"
      ],
      "metadata": {
        "id": "oZrfquKtyian"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### Which statistical test have you done to obtain P-Value?"
      ],
      "metadata": {
        "id": "Ou-I18pAyIpj"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Answer Here."
      ],
      "metadata": {
        "id": "s2U0kk00ygSB"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### Why did you choose the specific statistical test?"
      ],
      "metadata": {
        "id": "fF3858GYyt-u"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Answer Here."
      ],
      "metadata": {
        "id": "HO4K0gP5y3B4"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Hypothetical Statement - 2"
      ],
      "metadata": {
        "id": "4_0_7-oCpUZd"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 1. State Your research hypothesis as a null hypothesis and alternate hypothesis."
      ],
      "metadata": {
        "id": "hwyV_J3ipUZe"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Answer Here."
      ],
      "metadata": {
        "id": "FnpLGJ-4pUZe"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 2. Perform an appropriate statistical test."
      ],
      "metadata": {
        "id": "3yB-zSqbpUZe"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Perform Statistical Test to obtain P-Value"
      ],
      "metadata": {
        "id": "sWxdNTXNpUZe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### Which statistical test have you done to obtain P-Value?"
      ],
      "metadata": {
        "id": "dEUvejAfpUZe"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Answer Here."
      ],
      "metadata": {
        "id": "oLDrPz7HpUZf"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### Why did you choose the specific statistical test?"
      ],
      "metadata": {
        "id": "Fd15vwWVpUZf"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Answer Here."
      ],
      "metadata": {
        "id": "4xOGYyiBpUZf"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Hypothetical Statement - 3"
      ],
      "metadata": {
        "id": "bn_IUdTipZyH"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 1. State Your research hypothesis as a null hypothesis and alternate hypothesis."
      ],
      "metadata": {
        "id": "49K5P_iCpZyH"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Answer Here."
      ],
      "metadata": {
        "id": "7gWI5rT9pZyH"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 2. Perform an appropriate statistical test."
      ],
      "metadata": {
        "id": "Nff-vKELpZyI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Perform Statistical Test to obtain P-Value"
      ],
      "metadata": {
        "id": "s6AnJQjtpZyI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### Which statistical test have you done to obtain P-Value?"
      ],
      "metadata": {
        "id": "kLW572S8pZyI"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Answer Here."
      ],
      "metadata": {
        "id": "ytWJ8v15pZyI"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### Why did you choose the specific statistical test?"
      ],
      "metadata": {
        "id": "dWbDXHzopZyI"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Answer Here."
      ],
      "metadata": {
        "id": "M99G98V6pZyI"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## ***6. Feature Engineering & Data Pre-processing***"
      ],
      "metadata": {
        "id": "yLjJCtPM0KBk"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 1. Handling Missing Values"
      ],
      "metadata": {
        "id": "xiyOF9F70UgQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Handling Missing Values & Missing Value Imputation"
      ],
      "metadata": {
        "id": "iRsAHk1K0fpS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### What all missing value imputation techniques have you used and why did you use those techniques?"
      ],
      "metadata": {
        "id": "7wuGOrhz0itI"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Answer Here."
      ],
      "metadata": {
        "id": "1ixusLtI0pqI"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 2. Handling Outliers"
      ],
      "metadata": {
        "id": "id1riN9m0vUs"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Handling Outliers & Outlier treatments"
      ],
      "metadata": {
        "id": "M6w2CzZf04JK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### What all outlier treatment techniques have you used and why did you use those techniques?"
      ],
      "metadata": {
        "id": "578E2V7j08f6"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Answer Here."
      ],
      "metadata": {
        "id": "uGZz5OrT1HH-"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 3. Categorical Encoding"
      ],
      "metadata": {
        "id": "89xtkJwZ18nB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Encode your categorical columns"
      ],
      "metadata": {
        "id": "21JmIYMG2hEo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### What all categorical encoding techniques have you used & why did you use those techniques?"
      ],
      "metadata": {
        "id": "67NQN5KX2AMe"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Answer Here."
      ],
      "metadata": {
        "id": "UDaue5h32n_G"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 4. Textual Data Preprocessing\n",
        "(It's mandatory for textual dataset i.e., NLP, Sentiment Analysis, Text Clustering etc.)"
      ],
      "metadata": {
        "id": "Iwf50b-R2tYG"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 1. Expand Contraction"
      ],
      "metadata": {
        "id": "GMQiZwjn3iu7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Expand Contraction"
      ],
      "metadata": {
        "id": "PTouz10C3oNN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 2. Lower Casing"
      ],
      "metadata": {
        "id": "WVIkgGqN3qsr"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Lower Casing"
      ],
      "metadata": {
        "id": "88JnJ1jN3w7j"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 3. Removing Punctuations"
      ],
      "metadata": {
        "id": "XkPnILGE3zoT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Remove Punctuations"
      ],
      "metadata": {
        "id": "vqbBqNaA33c0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 4. Removing URLs & Removing words and digits contain digits."
      ],
      "metadata": {
        "id": "Hlsf0x5436Go"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Remove URLs & Remove words and digits contain digits"
      ],
      "metadata": {
        "id": "2sxKgKxu4Ip3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 5. Removing Stopwords & Removing White spaces"
      ],
      "metadata": {
        "id": "mT9DMSJo4nBL"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Remove Stopwords"
      ],
      "metadata": {
        "id": "T2LSJh154s8W"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Remove White spaces"
      ],
      "metadata": {
        "id": "EgLJGffy4vm0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 6. Rephrase Text"
      ],
      "metadata": {
        "id": "c49ITxTc407N"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Rephrase Text"
      ],
      "metadata": {
        "id": "foqY80Qu48N2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 7. Tokenization"
      ],
      "metadata": {
        "id": "OeJFEK0N496M"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Tokenization"
      ],
      "metadata": {
        "id": "ijx1rUOS5CUU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 8. Text Normalization"
      ],
      "metadata": {
        "id": "9ExmJH0g5HBk"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Normalizing Text (i.e., Stemming, Lemmatization etc.)"
      ],
      "metadata": {
        "id": "AIJ1a-Zc5PY8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### Which text normalization technique have you used and why?"
      ],
      "metadata": {
        "id": "cJNqERVU536h"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Answer Here."
      ],
      "metadata": {
        "id": "Z9jKVxE06BC1"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 9. Part of speech tagging"
      ],
      "metadata": {
        "id": "k5UmGsbsOxih"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# POS Taging"
      ],
      "metadata": {
        "id": "btT3ZJBAO6Ik"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 10. Text Vectorization"
      ],
      "metadata": {
        "id": "T0VqWOYE6DLQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Vectorizing Text"
      ],
      "metadata": {
        "id": "yBRtdhth6JDE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### Which text vectorization technique have you used and why?"
      ],
      "metadata": {
        "id": "qBMux9mC6MCf"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Answer Here."
      ],
      "metadata": {
        "id": "su2EnbCh6UKQ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 4. Feature Manipulation & Selection"
      ],
      "metadata": {
        "id": "-oLEiFgy-5Pf"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 1. Feature Manipulation"
      ],
      "metadata": {
        "id": "C74aWNz2AliB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Manipulate Features to minimize feature correlation and create new features"
      ],
      "metadata": {
        "id": "h1qC4yhBApWC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 2. Feature Selection"
      ],
      "metadata": {
        "id": "2DejudWSA-a0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Select your features wisely to avoid overfitting"
      ],
      "metadata": {
        "id": "YLhe8UmaBCEE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### What all feature selection methods have you used  and why?"
      ],
      "metadata": {
        "id": "pEMng2IbBLp7"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Answer Here."
      ],
      "metadata": {
        "id": "rb2Lh6Z8BgGs"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### Which all features you found important and why?"
      ],
      "metadata": {
        "id": "rAdphbQ9Bhjc"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Answer Here."
      ],
      "metadata": {
        "id": "fGgaEstsBnaf"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 5. Data Transformation"
      ],
      "metadata": {
        "id": "TNVZ9zx19K6k"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Do you think that your data needs to be transformed? If yes, which transformation have you used. Explain Why?"
      ],
      "metadata": {
        "id": "nqoHp30x9hH9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Transform Your data"
      ],
      "metadata": {
        "id": "I6quWQ1T9rtH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 6. Data Scaling"
      ],
      "metadata": {
        "id": "rMDnDkt2B6du"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Scaling your data"
      ],
      "metadata": {
        "id": "dL9LWpySC6x_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### Which method have you used to scale you data and why?"
      ],
      "metadata": {
        "id": "yiiVWRdJDDil"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 7. Dimesionality Reduction"
      ],
      "metadata": {
        "id": "1UUpS68QDMuG"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### Do you think that dimensionality reduction is needed? Explain Why?"
      ],
      "metadata": {
        "id": "kexQrXU-DjzY"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Answer Here."
      ],
      "metadata": {
        "id": "GGRlBsSGDtTQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# DImensionality Reduction (If needed)"
      ],
      "metadata": {
        "id": "kQfvxBBHDvCa"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### Which dimensionality reduction technique have you used and why? (If dimensionality reduction done on dataset.)"
      ],
      "metadata": {
        "id": "T5CmagL3EC8N"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Answer Here."
      ],
      "metadata": {
        "id": "ZKr75IDuEM7t"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 8. Data Splitting"
      ],
      "metadata": {
        "id": "BhH2vgX9EjGr"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Split your data to train and test. Choose Splitting ratio wisely."
      ],
      "metadata": {
        "id": "0CTyd2UwEyNM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### What data splitting ratio have you used and why?"
      ],
      "metadata": {
        "id": "qjKvONjwE8ra"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Answer Here."
      ],
      "metadata": {
        "id": "Y2lJ8cobFDb_"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 9. Handling Imbalanced Dataset"
      ],
      "metadata": {
        "id": "P1XJ9OREExlT"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### Do you think the dataset is imbalanced? Explain Why."
      ],
      "metadata": {
        "id": "VFOzZv6IFROw"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Answer Here."
      ],
      "metadata": {
        "id": "GeKDIv7pFgcC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Handling Imbalanced Dataset (If needed)"
      ],
      "metadata": {
        "id": "nQsRhhZLFiDs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### What technique did you use to handle the imbalance dataset and why? (If needed to be balanced)"
      ],
      "metadata": {
        "id": "TIqpNgepFxVj"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Answer Here."
      ],
      "metadata": {
        "id": "qbet1HwdGDTz"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## ***7. ML Model Implementation***"
      ],
      "metadata": {
        "id": "VfCC591jGiD4"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### ML Model - 1"
      ],
      "metadata": {
        "id": "OB4l2ZhMeS1U"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# ML Model - 1 Implementation\n",
        "\n",
        "# Fit the Algorithm\n",
        "\n",
        "# Predict on the model"
      ],
      "metadata": {
        "id": "7ebyywQieS1U"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 1. Explain the ML Model used and it's performance using Evaluation metric Score Chart."
      ],
      "metadata": {
        "id": "ArJBuiUVfxKd"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Visualizing evaluation Metric Score chart"
      ],
      "metadata": {
        "id": "rqD5ZohzfxKe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 2. Cross- Validation & Hyperparameter Tuning"
      ],
      "metadata": {
        "id": "4qY1EAkEfxKe"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# ML Model - 1 Implementation with hyperparameter optimization techniques (i.e., GridSearch CV, RandomSearch CV, Bayesian Optimization etc.)\n",
        "\n",
        "# Fit the Algorithm\n",
        "\n",
        "# Predict on the model"
      ],
      "metadata": {
        "id": "Dy61ujd6fxKe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### Which hyperparameter optimization technique have you used and why?"
      ],
      "metadata": {
        "id": "PiV4Ypx8fxKe"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Answer Here."
      ],
      "metadata": {
        "id": "negyGRa7fxKf"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### Have you seen any improvement? Note down the improvement with updates Evaluation metric Score Chart."
      ],
      "metadata": {
        "id": "TfvqoZmBfxKf"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Answer Here."
      ],
      "metadata": {
        "id": "OaLui8CcfxKf"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### ML Model - 2"
      ],
      "metadata": {
        "id": "dJ2tPlVmpsJ0"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 1. Explain the ML Model used and it's performance using Evaluation metric Score Chart."
      ],
      "metadata": {
        "id": "JWYfwnehpsJ1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Visualizing evaluation Metric Score chart"
      ],
      "metadata": {
        "id": "yEl-hgQWpsJ1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 2. Cross- Validation & Hyperparameter Tuning"
      ],
      "metadata": {
        "id": "-jK_YjpMpsJ2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# ML Model - 1 Implementation with hyperparameter optimization techniques (i.e., GridSearch CV, RandomSearch CV, Bayesian Optimization etc.)\n",
        "\n",
        "# Fit the Algorithm\n",
        "\n",
        "# Predict on the model"
      ],
      "metadata": {
        "id": "Dn0EOfS6psJ2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### Which hyperparameter optimization technique have you used and why?"
      ],
      "metadata": {
        "id": "HAih1iBOpsJ2"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Answer Here."
      ],
      "metadata": {
        "id": "9kBgjYcdpsJ2"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### Have you seen any improvement? Note down the improvement with updates Evaluation metric Score Chart."
      ],
      "metadata": {
        "id": "zVGeBEFhpsJ2"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Answer Here."
      ],
      "metadata": {
        "id": "74yRdG6UpsJ3"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 3. Explain each evaluation metric's indication towards business and the business impact pf the ML model used."
      ],
      "metadata": {
        "id": "bmKjuQ-FpsJ3"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Answer Here."
      ],
      "metadata": {
        "id": "BDKtOrBQpsJ3"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### ML Model - 3"
      ],
      "metadata": {
        "id": "Fze-IPXLpx6K"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# ML Model - 3 Implementation\n",
        "\n",
        "# Fit the Algorithm\n",
        "\n",
        "# Predict on the model"
      ],
      "metadata": {
        "id": "FFrSXAtrpx6M"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 1. Explain the ML Model used and it's performance using Evaluation metric Score Chart."
      ],
      "metadata": {
        "id": "7AN1z2sKpx6M"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Visualizing evaluation Metric Score chart"
      ],
      "metadata": {
        "id": "xIY4lxxGpx6M"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 2. Cross- Validation & Hyperparameter Tuning"
      ],
      "metadata": {
        "id": "9PIHJqyupx6M"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# ML Model - 3 Implementation with hyperparameter optimization techniques (i.e., GridSearch CV, RandomSearch CV, Bayesian Optimization etc.)\n",
        "\n",
        "# Fit the Algorithm\n",
        "\n",
        "# Predict on the model"
      ],
      "metadata": {
        "id": "eSVXuaSKpx6M"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### Which hyperparameter optimization technique have you used and why?"
      ],
      "metadata": {
        "id": "_-qAgymDpx6N"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Answer Here."
      ],
      "metadata": {
        "id": "lQMffxkwpx6N"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### Have you seen any improvement? Note down the improvement with updates Evaluation metric Score Chart."
      ],
      "metadata": {
        "id": "Z-hykwinpx6N"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Answer Here."
      ],
      "metadata": {
        "id": "MzVzZC6opx6N"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 1. Which Evaluation metrics did you consider for a positive business impact and why?"
      ],
      "metadata": {
        "id": "h_CCil-SKHpo"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Answer Here."
      ],
      "metadata": {
        "id": "jHVz9hHDKFms"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 2. Which ML model did you choose from the above created models as your final prediction model and why?"
      ],
      "metadata": {
        "id": "cBFFvTBNJzUa"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Answer Here."
      ],
      "metadata": {
        "id": "6ksF5Q1LKTVm"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 3. Explain the model which you have used and the feature importance using any model explainability tool?"
      ],
      "metadata": {
        "id": "HvGl1hHyA_VK"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Answer Here."
      ],
      "metadata": {
        "id": "YnvVTiIxBL-C"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## ***8.*** ***Future Work (Optional)***"
      ],
      "metadata": {
        "id": "EyNgTHvd2WFk"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 1. Save the best performing ml model in a pickle file or joblib file format for deployment process.\n"
      ],
      "metadata": {
        "id": "KH5McJBi2d8v"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Save the File"
      ],
      "metadata": {
        "id": "bQIANRl32f4J"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 2. Again Load the saved model file and try to predict unseen data for a sanity check.\n"
      ],
      "metadata": {
        "id": "iW_Lq9qf2h6X"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Load the File and predict unseen data."
      ],
      "metadata": {
        "id": "oEXk9ydD2nVC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### ***Congrats! Your model is successfully created and ready for deployment on a live server for a real user interaction !!!***"
      ],
      "metadata": {
        "id": "-Kee-DAl2viO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Conclusion**"
      ],
      "metadata": {
        "id": "gCX9965dhzqZ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Write the conclusion here."
      ],
      "metadata": {
        "id": "Fjb1IsQkh3yE"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### ***Hurrah! You have successfully completed your Machine Learning Capstone Project !!!***"
      ],
      "metadata": {
        "id": "gIfDvo9L0UH2"
      }
    }
  ]
}